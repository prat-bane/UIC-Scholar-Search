<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bi-LSTM-CRF Sequence Labeling for Keyphrase Extraction from Scholarly Documents</title>
				<funder ref="#_RQMdgfQ #_7fAEeun #_JKre68w #_sJWgYKN">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>ACM</publisher>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2019-05-13">2019-05-13</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rabah</forename><surname>Alzaidy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Cornelia</forename><surname>Caragea</surname></persName>
							<email>cornelia@uic.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Pennsylvania State University</orgName>
								<orgName type="institution" key="instit2">University of Illinois at Chicago</orgName>
								<orgName type="institution" key="instit3">Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">C</forename><forename type="middle">Lee</forename><surname>Giles</surname></persName>
							<email>giles@ist.psu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Pennsylvania State University</orgName>
								<orgName type="institution" key="instit2">University of Illinois at Chicago</orgName>
								<orgName type="institution" key="instit3">Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Bi-LSTM-CRF Sequence Labeling for Keyphrase Extraction from Scholarly Documents</title>
					</analytic>
					<monogr>
						<title level="m">The World Wide Web Conference</title>
						<imprint>
							<publisher>ACM</publisher>
							<biblScope unit="page" from="2551" to="2557"/>
							<date type="published" when="2019-05-13" />
						</imprint>
					</monogr>
					<idno type="MD5">D6B14E474F2B5D64C4668401CC553B2F</idno>
					<idno type="DOI">10.1145/3308558.3313642</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-11-27T03:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Computing methodologies → Natural language processing; Information extraction Keyphrase extraction</term>
					<term>sequence labeling</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we address the keyphrase extraction problem as sequence labeling and propose a model that jointly exploits the complementary strengths of Conditional Random Fields that capture label dependencies through a transition parameter matrix consisting of the transition probabilities from one label to the neighboring label, and Bidirectional Long Short Term Memory networks that capture hidden semantics in text through the long distance dependencies. Our results on three datasets of scholarly documents show that the proposed model substantially outperforms strong baselines and previous approaches for keyphrase extraction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Keyphrase extraction is a key natural language processing task aimed at automatically extracting descriptive phrases or words from a document <ref type="bibr" target="#b16">[17]</ref>. Keyphrases (also referred as keywords) provide a brief summary of the content of a document. The importance of keyphrases has been widely recognized in many downstream applications such as query formulation, document clustering, classification, recommendation, indexing, and summarization <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>Most of the existing works on automatic keyphrase extraction focus on supervised and unsupervised approaches. The unsupervised approaches use ranking techniques to rank phrases based on the aggregated "informativeness" scores of the individual words comprising a phrase. Graph-based ranking algorithms that are applied to the word graph representation of a document are the most prevalent in this category. To construct the graph, each candidate word in a document (i.e., a word with certain part-of-speech tags) is mapped to a node and connecting edges represent the association patterns among the candidate words. The scores of individual words are estimated using various graph centrality measures such as PageRank <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>The supervised approaches use binary classification to label candidate phrases positively (as keyphrases) or negatively (as nonkeyphrases), based on a set of linguistic and statistical features such as tf-idf, part of speech (POS) tags, and the position of phrases in documents. Supervised keyphrase extraction allows for expressive feature design and is reported to often outperform unsupervised methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref>. Two major limitations of these supervised approaches are: (1) they classify the labels of each candidate phrase independently, while completely ignoring the dependencies that could potentially exist between neighboring labels; and (2) they do not incorporate the hidden semantics in the input text.</p><p>More recently, Gollapalli et al. <ref type="bibr" target="#b12">[13]</ref> formulated keyphrase extraction as sequence labeling and showed that using linear-chain Conditional Random Fields can improve the performance over baseline models for this task. However, the approach in <ref type="bibr" target="#b12">[13]</ref> does not explicitly take into account the long-term dependencies and semantic relationships hidden in text. Figure <ref type="figure" target="#fig_0">1</ref> shows examples of long-term dependency patterns and semantic relationships hidden in text from a research paper published in the ACM KDD conference, e.g., the phrase "we describe" is followed by (and indicative of) keyphrases, whereas the terms "Conditional Random Field" and "discriminatively-trained model" are semantically related. We posit that a deep understanding of the text is required in order to correctly identify keyphrases for a document.</p><p>To this end, we address keyphrase extraction as a sequence labeling problem, using research papers as a case study, and precisely aim to capture both the semantics of document contexts as well as the dependencies among the labels of neighboring words in order to overcome the limitations in previous approaches. Specifically, we explore a neural learning model, called Bi-LSTM-CRF, that combines a bi-directional Long Short-Term Memory (Bi-LSTM) layer to model the sequential text data with a Conditional Random Field (CRF) layer to model dependencies in the output <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30]</ref>. The result of this extraction task will aid indexing of documents in scholarly document collections, and hence, will lead to improved organization, search, retrieval, and recommendation of scientific documents. In summary, our contributions are as follows:</p><p>• We explore a neural learning model for keyphrase extraction from scholarly documents that combines the complementary strengths of Bi-LSTM and CRF. In the combined model, the input and output layers are not directly connected as in CRF, but instead a Bi-LSTM layer is inserted between them to exploit the long term dependencies in the text.</p><p>A Unified Approach for Schema Matching, Coreference and Canonicalization by Michael L. Wick, Khashayar Rohanimanesh, Karl Schultz and Andrew McCallum The automatic consolidation of database records from many heterogeneous sources into a single repository requires solving several information integration tasks. Although tasks such as coreference, schema matching, and canonicalization are closely related, they are most commonly studied in isolation. Systems that do tackle multiple integration problems traditionally solve each independently, allowing errors to propagate from one task to another. In this paper, we describe a discriminatively-trained model that reasons about schema matching, coreference, and canonicalization jointly. [...] Author-annotated keyphrases: Data Integration, Coreference, Schema Matching, Canonicalization, Conditional Random Field, Weighted Logic • We conduct a thorough evaluation to examine the role of each layer in the model.</p><p>To our knowledge, there is no study that demonstrates an ablation experiment that compares, with this clarity, the role of input dependencies and label dependencies in keyphrase extraction from research papers. • We show empirically on three datasets of research papers that the Bi-LSTM-CRF model outperforms strong baselines and previous works on the keyphrase extraction task. • We investigate the performance of Bi-LSTM-CRF at document and sentence level and show that a document level model that captures a broader context is more accurate than a sentence level model. To our knowledge, this has not been addressed before, especially, not on a large scale dataset as we do in this paper.</p><p>In the next section, we describe related work on keyphrase extraction. We then introduce the neural learning model in Section 3, followed by Section 4 that presents our evaluation setup. Finally, we discuss our experimental results in Section 5 before we conclude the paper and touch on future directions in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Keyphrase extraction has been the focus of many studies. These studies generally adopt a two phase approach. In the first phase, candidate words or phrases are extracted from the text using heuristics such as POS patterns for words or n-grams <ref type="bibr" target="#b19">[20]</ref>. In the second phase, the candidate phrases are predicted as keyphrases or nonkeyphrases, using both supervised and unsupervised approaches. In the supervised approaches, the prediction is done based on a selection of features, e.g., POS tags, tf-idf scores, and position information, used in conjunction with machine learning classifiers <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39]</ref>. Traditional features were also combined with features extracted from external sources such as WordNet and Wikipedia <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b31">32]</ref> or from various neighborhoods, e.g., a document's citation network or a webpage's hyper-link network <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Unsupervised approaches include phrase scoring methods based on measures such as tf-idf and topic proportions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b45">46]</ref>, graphbased ranking using centrality measures <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41]</ref>, and keyphrase selection from topics detected using topic modeling <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40]</ref>. In this context, several extensions of PageRank and personalized PageRank have been proposed that make use of a document's citation network <ref type="bibr" target="#b11">[12]</ref> or that bias the random walk based on the words' positions in text <ref type="bibr" target="#b9">[10]</ref> or the words' topic distribution <ref type="bibr" target="#b26">[27]</ref>. In order to add semantic relatedness between the words in a word graph, Martinez-Romo et al. <ref type="bibr" target="#b30">[31]</ref> used information from WordNet.</p><p>The best performing SemEval 2010 system used term frequency thresholds to filter out phrases that are unlikely to be keyphrases, where the thresholds were estimated from the data <ref type="bibr" target="#b8">[9]</ref>. The candidate phrases were ranked using the tf-idf model in conjunction with a boosting factor which aims at reducing the bias towards single word terms. Danesh et al. <ref type="bibr" target="#b7">[8]</ref> computed an initial weight for each phrase based on a combination of the tf-idf score and the first position of a phrase in a document. Phrases and their initial weights were then incorporated into a graph-based algorithm, which produces the final ranking of keyphrases. Adar and Datta <ref type="bibr" target="#b0">[1]</ref> extracted keyphrases by mining abbreviations from scientific literature and built a semantic hierarchical keyphrase database. Many of the above approaches, both supervised and unsupervised, are compared and analyzed in a survey by Hasan and Ng <ref type="bibr" target="#b16">[17]</ref>.</p><p>Neural networks have started to be incorporated into models for keyphrase extraction. For example, Wang et al. <ref type="bibr" target="#b41">[42]</ref> investigate word embeddings to measure the relatedness between words in graph-based models. A Recurrent Neural Network (RNN) based approach is proposed by Zhang et al. <ref type="bibr" target="#b44">[45]</ref> to identify keyphrases in Twitter data. The model addresses the problem as sequence labeling for very short text, where a joint-layer RNN is used to capture the semantic dependencies in the input sequence, but does not address the dependencies in the labels. In our work, we capture the flow of scientific writing and the dependencies between keyphrases and the other words in the text that may not necessarily exist between hashtags and the text of the tweets. Augenstein and Søgaard <ref type="bibr" target="#b1">[2]</ref> treated keyphrase extraction as a multi-task learning problem and applied RNNs to classify keyphrase boundaries. Inspired from work in machine translation, Meng et al. <ref type="bibr" target="#b32">[33]</ref> focused on keyphrase generation (rather than keyphrase extraction) and addressed it as a sequence to sequence learning problem with a copying mechanism, where the sequence of words in a document is used to generate a sequence of keyphrases. An Encoder-Decoder RNN, originally proposed by Cho et al. <ref type="bibr" target="#b6">[7]</ref>, was used to generate the keyphrase sequences. A variation of the model, which performs better than the Encoder-Decoder RNN, includes a copying mechanism to identify keyphrases that occur rarely in the text. Unlike Meng et al. <ref type="bibr" target="#b32">[33]</ref>, we focus on keyphrase extraction, i.e., extracting only words that are present in text, and not keyphrase generation, which outputs words that may or may not be present in text. We use the Encoder-Decoder RNN with the copying mechanism as one of our baselines.</p><p>Sequence labeling models for keyphrase extraction have shown promising results in recent studies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b43">44]</ref>. For example, Gollapalli et al. <ref type="bibr" target="#b12">[13]</ref> trained Conditional Random Fields (CRFs) to extract keyphrases from scholarly documents, using features such as tf-idf and POS tags to predict a label for each token position in a document as being a keyphrase token (KP) or not (Non-KP). This CRF model is able to capture the dependencies in previous and future tags in the label sequence, however, the semantic dependencies in the input sequence, i.e., the text, are not incorporated.</p><formula xml:id="formula_0">x1 x2 xn-1 xn input • • • output • • • y1 y2 yn-1 yn</formula><p>Recently, a sequence labeling framework that takes into consideration both types of dependencies has been explored on a variety of NLP tasks such as part-of-speech tagging, noun phrase chunking, and named entity recognition <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30]</ref>. This framework combines a bidirectional LSTM (Bi-LSTM) network as the first layer to capture sequential text dependencies with a second CRF layer to capture label dependencies. In our work, we identify the limitations of each independent model (Bi-LSTM and CRF) and explore their combination on keyphrase extraction from scholarly documents. To our knowledge, in the context of keyphrase extraction from scholarly documents, we are the first to simultaneously capture the semantics of document contexts and long-range dependencies in text by modeling the sequential text data as well as the dependencies among the labels of neighboring words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>In this section, we formulate keyphrase extraction as a sequence labeling task and present the details of the Bi-LSTM-CRF model and its constituent components, CRF and Bi-LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Keyphrase extraction can be formulated as a sequence labeling task as follows: Given an input sequence x = {x 1 , • • • , x n }, where each x i represents the input vector of the i t h word, predict a sequence of labels y = {y 1 , • • • , y n }, one label for each word in the input, where each label y i is KP (keyphrase word) or Non-KP (not keyphrase word). The sequence labeling formulation of our task takes into account the correlations between neighboring labels and allows to jointly decode the best sequence of labels for the input sequence, rather than decoding each label independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Conditional Random Fields</head><p>Conditional Random Field (CRF) introduced by Lafferty et al. <ref type="bibr" target="#b23">[24]</ref> has been successfully used in many sequence labeling tasks and we use it here to jointly model the sequence of labels for our keyphrase extraction task. The conditional probability distribution over the label sequence y given x defined by CRF has the form:</p><formula xml:id="formula_1">p(y|x; W, b) ∝ exp n i=1 W T y i -1 ,y i x i + b y i -1 ,y i</formula><p>where W y i -1 ,y i and b y i -1 ,y i are model parameters (weight vector and bias) corresponding to the neighboring labels (y i-1 , y i ).</p><p>For training a CRF model, we estimate model parameters W and b from a training dataset D = {(x (j ) , y (j ) )} N j=1 by maximizing the log-likelihood given by:</p><formula xml:id="formula_2">x1 x2 xn-1 xn input • • • hidden backward • • • • • • forward output • • • y1 y2 yn-1 yn</formula><formula xml:id="formula_3">L(W, b) = N j=1 log p(y (j ) |x (j ) ; W, b)</formula><p>To find the best sequence path during decoding, the optimal sequence y that maximizes the likelihood, is computed using the Viterbi decoding: </p><formula xml:id="formula_4">y * = argmax y ∈Y (x) p(y|x; W, b)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Bi-LSTM</head><p>Long Short Term Memory networks (LSTMs) are a special type of Recurrent Neural Networks (RNNs) and are designed to overcome the gradient vanishing problem of RNNs. In particular, LSTMs have additional memory cells, which store memory from long distance terms <ref type="bibr" target="#b17">[18]</ref>. Because LSTMs are capable of preserving information over previous inputs of a sequence into the current input state, they have been a natural choice for applications involving temporal and sequence data such as speech recognition, language modeling and translation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b34">35]</ref>. The structure of an LSTM includes an input layer, a hidden layer and an output layer. An LSTM unit at time t consists of sub-unit-inputs (i t ), output (o t ), forget gates (f t ) and memory cell (c t ). The updates at time t are then:</p><formula xml:id="formula_5">i t = σ (W (i ) h t -1 + U (i ) x t + b (i ) ) f t = σ (W (f ) h t -1 + U (f ) x t + b (f ) ) o t = σ (W (o) h t -1 + U (o) x t + b (o) ) ct = tanh(W (c ) h t -1 + U (c ) x t + b (c ) ) c t = f t ⊙ c (t -1) + i t ⊙ ct h t = o t ⊙ tanh(c t )</formula><p>Here, σ is the element-wise logistic sigmoid function and ⊙ is the Hadamard product; x t is the input vector at time t, i.e., the word embedding in our case, and h t is the hidden state that stores sequential information up to time t. W, U, and b are model parameters to be estimated during training.</p><p>In a forward LSTM network, the hidden state h t stores information only from the past. To capture the flow of information both ways, we use a bi-directional LSTM network, which consists of a forward hidden layer and a backward hidden layer. Figure <ref type="figure" target="#fig_2">3</ref> shows the structure of a Bi-LSTM network. Here, the nodes in the hidden layer are connected which is how long distance information is maintained in the matrix weights.  </p><formula xml:id="formula_6">• • • hidden backward • • • • • • forward output • • • y1 y2 yn-1 yn</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Bi-LSTM-CRF</head><p>In order to build a sequence labeling model that incorporates long distance information over a sequence of input as well as information on the output sequence, we combine a Bi-LSTM network with a CRF network. The network architecture is shown in Figure <ref type="figure" target="#fig_4">4</ref>. As shown in the figure, the first layer of the model is a Bi-LSTM network with the purpose of capturing the semantics of the input text sequence. The output of the Bi-LSTM layer is passed to a CRF layer that produces a probability distribution over the tag sequence using the dependencies among labels of the entire sequence. In order to find the best sequence of labels for an input sequence, the Viterbi algorithm is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION SETTING</head><p>To evaluate the performance of our proposed model, we conduct a wide range of experiments. In this section, we describe the datasets used for training and evaluation, discuss hyper-parameters, baselines and the evaluation measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We use three different datasets of scientific documents for our evaluation purpose. The first dataset was made available by Meng et al. <ref type="bibr" target="#b32">[33]</ref> and was collected by crawling the metadata of papers from several online digital libraries such as ACM, WebofScience, and Wiley. The dataset contains metadata for 567, 830 papers with a clear split as train, validation, and test sets provided by the authors, as follows: 527, 830 were used for model training, 20, 000 were used for parameter tuning, and the remaining 20, 000 were used for model evaluation. We refer to these sets as kp527k, kp20k-validation (or kp20k-v), and kp20k-test (or simply kp20k), respectively. The second and third datasets were made available by Gollapalli and Caragea <ref type="bibr" target="#b11">[12]</ref> and were compiled from the CiteSeerX digital library. These datasets contain metadata for research papers from the proceedings of two top-tier data mining and machine learning conferences, i.e., the ACM Conference on Knowledge Discovery and Data Mining (referred as KDD) and the World Wide Web Conference (referred as WWW).</p><p>The metadata of each paper from all three datasets above consist of titles, abstracts, and author-assigned keyphrases. The title and abstract of each paper are used to extract keyphrases, whereas the author-input keyphrases are used as gold-standard for evaluation. In experiments, for all models, we use kp527k for model training, kp20k-validation for parameter tuning, and kp20k, KDD, and WWW as three independent test sets for model evaluation. Note that we removed from KDD and WWW the entries that occur also in kp527k-train and kp20k-validation to avoid the overlap between train, validation, and test sets. Table ?? shows the statistics of these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Since we use a sequence labeling formulation of the keyphrase extraction problem, the abstract/keyphrases data pairs are converted such that each document is a sequence of word tokens, each with a positive (KP) label if it occurs in a keyphrase, or with a negative (Non-KP) label, otherwise (consistent with <ref type="bibr" target="#b12">[13]</ref>). We train four network models: Bi-LSTM-CRF, CRF, Bi-LSTM, and LSTM. Each sentence from a document is passed to a network as the input sequence. To evaluate performance and runtime differences, we also train Bi-LSTM-CRF models with the entire document as the input sequence (instead of each sentence at a time).</p><p>We use word embeddings as input to the above four models. The word embeddings are initialized with 100-dimension Glove pre-trained embedding vectors <ref type="bibr" target="#b36">[37]</ref>. For all models, we use a single 100-dimension hidden layer. The LSTM, Bi-LSTM, and Bi-LSTM-CRF are optimized during training using stochastic gradient descent with learning rate η t = η 0 1+ρ t , where initial learning rate η 0 = 10 -<ref type="foot" target="#foot_1">foot_1</ref> and decay ratio ρ t = 0.5. Gradient clipping of 5.0 is used to prevent the gradient from overflows during back-propagation. In addition, we use dropout to avoid over-fitting. We select the model with the best F1 score on the validation set, kp20k-validation.</p><p>Our implementation of the network models is based on a modified version of the implementation developed by <ref type="bibr" target="#b25">[26]</ref>. <ref type="foot" target="#foot_0">1</ref> Publicly available implementations were used for the other baselines <ref type="bibr" target="#b32">[33]</ref>. 2   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines and Evaluation Measures</head><p>We contrast the Bi-LSTM-CRF network with three baselines: CRF, forward LSTM, and Bi-LSTM, and several previous models: copy-RNN <ref type="bibr" target="#b32">[33]</ref>, KEA <ref type="bibr" target="#b10">[11]</ref>, Tf-Idf, TextRank <ref type="bibr" target="#b33">[34]</ref> and SingleRank <ref type="bibr" target="#b40">[41]</ref>. Consistent with previous works, we evaluate the predictions of each model against the author-input keyphrases, which are available with each document, i.e., the gold standard, and report Precision, Recall, and F1-score. For model comparison in the next section, we focus the discussion of our results in terms of the F1-score, which is the harmonic mean of Precision and Recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS AND OBSERVATIONS</head><p>We now describe the evaluation performance of the Bi-LSTM-CRF model for extracting keyphrases from research papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Performance of Bi-LSTM-CRF for Keyphrase Extraction</head><p>First, we evaluate the performance of Bi-LSTM-CRF in an ablation experiment to determine the role played by each component in extracting keyphrases from research papers. Specifically, we compare the Bi-LSTM-CRF model with a bidirectional LSTM, a forward LSTM, and a CRF model (by removing one component at a time from the full Bi-LSTM-CRF model).</p><p>Table <ref type="table" target="#tab_3">2</ref> shows the results of this comparison. As can be seen from the table, Bi-LSTM-CRF consistently achieves the best results on all three datasets in terms of the F1-score. For example, on the kp20k dataset, Bi-LSTM-CRF achieves an F1-score of 35.63%, whereas CRF and Bi-LSTM achieve an F1-score of 17.46% and 16.75%, respectively. Interestingly, removing the Bi-LSTM component and using only a CRF model, we notice a steep drop in recall on all three datasets. For example, on kp20k, Bi-LSTM-CRF achieves a recall of 24.66%, whereas CRF alone achieves a recall of only 10.04%. This result demonstrates that leveraging the long distance semantic dependencies from text through the Bi-LSTM component of the full model is beneficial for correctly extracting a larger fraction of gold keyphrases. The CRF model alone generally achieves a similar or small increase in precision compared with the Bi-LSTM-CRF model. For example, on WWW, the precision values of Bi-LSTM-CRF and CRF are 64.33% and 64.89%, respectively, whereas on the kp20k dataset, these values are 64.19% and 66.67%, respectively.</p><p>On the other hand, removing the CRF component and using only an LSTM model (either bidirectional or forward LSTM) yields a consistent and substantial improvement in recall on all datasets over both CRF and Bi-LSTM-CRF at the expense of a dramatic drop in precision, consistently across all datasets (see Table <ref type="table" target="#tab_3">2</ref>). That is, on kp20k, the precision drops from 64.19% (achieved by Bi-LSTM-CRF) to an unacceptable value of 9.41% (obtained by LSTM alone), whereas the recall increases from 24.66% to 78.43%, obtained by Bi-LSTM-CRF and LSTM, respectively. This result indicates that LSTM is powerful in capturing the deep semantics of the text, and is</p><p>KP Non-KP KP Non-KP 0.4211 0.5789 0.0267 0.9733 KP Non-KP KP Non-KP 0.5132 0.4868 0.0358 0.9642 KP Non-KP KP Non-KP 0.5527 0.4473 0.0387 0.9613 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8 (a) Bi-LSTM-CRF KP Non-KP KP Non-KP 0.2087 0.7913 0.0145 0.9855 KP Non-KP KP Non-KP 0.3266 0.6734 0.0228 0.9772 KP Non-KP KP Non-KP 0.2964 0.7036 0.0218 0.9782 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8 (b) CRF Table <ref type="table" target="#tab_3">2</ref> shows also that the performance of bidirectional-LSTM is very similar to that of forward LSTM. Intuitively, this makes sense since, in scientific papers, often dependencies occur in a forward fashion, e.g., patterns such as "we propose/study/explore/describe" precede keyphrases.</p><p>Moreover, from Table <ref type="table" target="#tab_3">2</ref>, we can also see that the performance on the WWW and KDD datasets is higher than that on the kp20k dataset. The lower performance on kp20k could be due to a larger spectrum of venues, author writing styles and author keyphrase annotations, whereas WWW and KDD are specialized datasets of papers from the data mining and machine learning communities.</p><p>Figures <ref type="figure" target="#fig_6">5a</ref> and <ref type="figure" target="#fig_6">5b</ref> show the confusion matrices of Bi-LSTM-CRF and CRF, on all three datasets. Each matrix is represented as a heat map, i.e., the darker the color, the higher the value at that position. An accuracy of 100% will be represented by a matrix with dark blue blocks on the main diagonal and white blocks off diagonal. As can be seen from the figures, the Bi-LSTM-CRF model that incorporates </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baseline Comparisons</head><p>Second, we compare the performance of Bi-LSTM-CRF with existing state-of-the-art models including supervised, unsupervised as well as deep learning models. The unsupervised models are Tf-Idf, TextRank <ref type="bibr" target="#b33">[34]</ref>, and SingleRank <ref type="bibr" target="#b40">[41]</ref>. The supervised model is KEA 3 <ref type="bibr" target="#b42">[43]</ref>, and the deep learning model is the recently proposed copyRNN 4 <ref type="bibr" target="#b32">[33]</ref>, which is a sequence to sequence learning model based on an RNN Encoder-Decoder framework <ref type="bibr" target="#b6">[7]</ref>, combined with a copying mechanism. The Bi-LSTM-CRF, copyRNN, and KEA models are all trained on the kp527k dataset. For the unsupervised models and the sequence to sequence learning model, we report the performance at top 5 predicted keyphrases since top-5 showed highest performance in previous works for these models. Table <ref type="table" target="#tab_4">3</ref> shows the results of this comparison. The Bi-LSTM-CRF model outperforms all baselines in terms of the F1-score, on all three datasets. More notably, Bi-LSTM-CRF outperforms the copyRNN model in precision on all datasets, yet is slightly worse in the recall score on the kp20k dataset. For example, on the kp20k, Bi-LSTM-CRF achieves an F1-score of 35.63% as compared with 33.29% achieved by copyRNN. The precision increases from 27.71% (obtained by copyRNN) to 64.19% (obtained by Bi-LSTM-CRF) at the expense of a drop in recall from 41.79% (obtained by copyRNN) to 24.66% (obtained by Bi-LSTM-CRF).</p><p>This result is consistent with our findings in the previous section regarding the role of CRF in improving precision. Although both models use variants of RNNs to capture the semantics of the input sequence, by integrating the learning of phrasal structure into the model itself, via the CRF layer, we get higher performance than applying beam search to a decoded sequence after the learning phase, as is done in the copyRNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Sentence vs. Document Level Input</head><p>Third, we conduct an experiment to examine the effect of the type of input sequence used to the models. Specifically, we compare the model where the input sequence consists of the entire document with the model where the input sequence consists of each sentence.   <ref type="table" target="#tab_5">4</ref> shows the results of this comparison for document vs. sentence level Bi-LSTM-CRF, on the kp20k dataset. As can be seen from the table, the performance of the Bi-LSTM-CRF that takes as input the entire content (all sentences at once) is higher than that of the Bi-LSTM-CRF that takes as input each sentence at a time. Precisely, precision, recall, and F1 values for the document-level Bi-LSTM-CRF show improvement by up to 6% over the sentencelevel Bi-LSTM-CRF. This is likely due to the increased information captured by the LSTM layer (through the larger context) when using the entire content as opposed to each sentence.</p><p>A common drawback with increasing input sequence length for a recurrent neural network is that computing the gradients during back-propagation takes longer, which in turn increases the time it takes to train the model. For example, in our setting, training the Bi-LSTM-CRF model on the kp527k dataset at the document level took on average 22.1 hours for only five epochs, where the average input sequence size is 147 tokens, whereas when training Bi-LSTM-CRF using the sentence-level, it took only 2.7 hours, where the average number of tokens in a sentence is 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>In this paper, we formulated the keyphrase extraction task as sequence labeling and proposed to use a Bi-LSTM-CRF model that incorporates dependencies among both the input and output sequences. To our knowledge, this is the first work to propose a neural network based sequence labeling model for keyphrase extraction from scholarly documents. The model takes advantage of the ability of Bi-LSTM to capture long distance semantic information from the input sequence and the ability of CRF to capture dependencies from the label sequence. Experimental results on three datasets showed that the proposed Bi-LSTM-CRF model that takes into account both of these dependencies play an important role in the keyphrase extraction performance.</p><p>As future work, it would be interesting to integrate the relationships between documents such as those available from a citation network. Another interesting direction would be to extend keyphrase extraction approaches to other scholarly domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The title and abstract of a paper by Wick et al. (2008) and the author-input keyphrases for the paper. Cyan bold phrases represent the gold-standard (author-annotated) keyphrases for the document. Red italic phrases represent long-term dependency patterns or semantic relationships with the gold-standard keyphrases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Layers in a CRF network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Layers in a Bi-LSTM network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2</head><label>2</label><figDesc>Figure 2 depicts the layout of a simple CRF network. As shown in the figure, nodes in the output layer are connected, enabling the model to capture dependencies in the sequence of labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Layers in a Bi-LSTM-CRF network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Keyphrase extraction confusion matrices on kp20k (left), WWW (middle), and KDD (right) (results reported at word-level). The darker the blue on the main diagonal, the more accurate the model is. able to achieve a high recall. This result also indicates that the CRF component of the full Bi-LSTM-CRF model successfully captures the label dependencies among the output labels for identifying keyphrases and contributes the most towards the full model's precision. Thus, exploiting dependencies in both the textual content of a document and the sequence of labels through the combination of an LSTM model with a CRF model yields improved results over CRF and LSTM models alone, quantified by a much larger F1-score.Table2shows also that the performance of bidirectional-LSTM is very similar to that of forward LSTM. Intuitively, this makes sense since, in scientific papers, often dependencies occur in a forward fashion, e.g., patterns such as "we propose/study/explore/describe" precede keyphrases.Moreover, from Table2, we can also see that the performance on the WWW and KDD datasets is higher than that on the kp20k dataset. The lower performance on kp20k could be due to a larger spectrum of venues, author writing styles and author keyphrase annotations, whereas WWW and KDD are specialized datasets of papers from the data mining and machine learning communities.Figures5a and 5bshow the confusion matrices of Bi-LSTM-CRF and CRF, on all three datasets. Each matrix is represented as a heat map, i.e., the darker the color, the higher the value at that position. An accuracy of 100% will be represented by a matrix with dark blue blocks on the main diagonal and white blocks off diagonal. As can be seen from the figures, the Bi-LSTM-CRF model that incorporates</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><figDesc>3 http://www.nzdl.org/Kea/Download/Kea-4.0.zip 4 https://github.com/memray/seq2seq-keyphrase</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics.</figDesc><table><row><cell></cell><cell>Statistics</cell><cell></cell><cell></cell><cell cols="2">Training Validation kp527k kp20k-v</cell><cell>Testing kp20k WWW</cell><cell>KDD</cell></row><row><cell></cell><cell cols="2">Number of documents</cell><cell></cell><cell>527,830</cell><cell>20,000</cell><cell>20,000</cell><cell>1,330</cell><cell>755</cell></row><row><cell></cell><cell cols="2">Number of sentences</cell><cell></cell><cell>4,686,986</cell><cell>176,930</cell><cell>177,278</cell><cell>12,288</cell><cell>7,768</cell></row><row><cell></cell><cell cols="2">Number of tokens in total</cell><cell></cell><cell>78,441,075</cell><cell cols="2">2,948,609 2,971,668 200,704 132,728</cell></row><row><cell></cell><cell cols="3">Number of tokens in keyphrases</cell><cell>5,458,743</cell><cell>205,586</cell><cell>207,073</cell><cell>12,181</cell><cell>6,119</cell></row><row><cell></cell><cell cols="2">Number of keyphrases</cell><cell></cell><cell>2,806,381</cell><cell>106,181</cell><cell>105,523</cell><cell>6,405</cell><cell>3,093</cell></row><row><cell>input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>x1</cell><cell>x2</cell><cell>xn-1</cell><cell>xn</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results (Pr, Re, F1) of Bi-LSTM-CRF in an ablation experiment on the kp20k, WWW, and KDD datasets.</figDesc><table><row><cell></cell><cell></cell><cell>kp20k</cell><cell></cell><cell></cell><cell>WWW</cell><cell></cell><cell></cell><cell>KDD</cell><cell></cell></row><row><cell>Method</cell><cell>Pr%</cell><cell>Re%</cell><cell>F1%</cell><cell>Pr%</cell><cell>Re%</cell><cell>F1%</cell><cell>Pr%</cell><cell>Re%</cell><cell>F1%</cell></row><row><cell cols="4">Bi-LSTM-CRF 64.19 24.66 35.63</cell><cell cols="6">64.33 28.43 39.43 57.83 31.85 41.08</cell></row><row><cell>CRF</cell><cell cols="6">66.67 10.04 17.46 64.89 22.11 32.98</cell><cell cols="3">55.76 18.69 27.99</cell></row><row><cell>Bi-LSTM</cell><cell cols="3">9.41 76.24 16.75</cell><cell cols="3">9.53 76.76 16.96</cell><cell cols="3">8.15 75.93 14.71</cell></row><row><cell>LSTM</cell><cell cols="3">9.41 78.43 16.81</cell><cell cols="3">9.51 75.71 16.90</cell><cell cols="3">8.13 75.38 14.68</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of Bi-LSTM-CRF with previous approaches on kp20k, WWW, and KDD datasets.</figDesc><table><row><cell></cell><cell></cell><cell>kp20k</cell><cell></cell><cell></cell><cell>WWW</cell><cell></cell><cell></cell><cell>KDD</cell><cell></cell></row><row><cell>Method</cell><cell>Pr%</cell><cell>Re%</cell><cell>F1%</cell><cell>Pr%</cell><cell>Re%</cell><cell>F1%</cell><cell>Pr%</cell><cell>Re%</cell><cell>F1%</cell></row><row><cell>Bi-LSTM-CRF</cell><cell cols="9">64.19 24.66 35.63 64.33 28.43 39.43 57.83 31.85 41.08</cell></row><row><cell>copyRNN @5</cell><cell cols="3">27.71 41.79 33.29</cell><cell cols="3">11.47 14.72 12.89</cell><cell>8.59</cell><cell>11.8</cell><cell>9.94</cell></row><row><cell>Tf-Idf @5</cell><cell cols="3">8.97 13.49 10.77</cell><cell cols="2">8.90 10.00</cell><cell>9.40</cell><cell cols="2">8.30 10.20</cell><cell>9.20</cell></row><row><cell>TextRank @5</cell><cell cols="3">15.29 23.01 18.37</cell><cell>5.80</cell><cell>7.10</cell><cell>6.20</cell><cell>5.10</cell><cell>6.50</cell><cell>5.60</cell></row><row><cell>SingleRank @5</cell><cell cols="3">8.42 12.70 10.14</cell><cell cols="2">8.80 10.90</cell><cell>9.50</cell><cell cols="2">7.70 10.30</cell><cell>8.60</cell></row><row><cell>KEA</cell><cell cols="3">15.14 22.78 18.19</cell><cell cols="3">13.57 15.25 13.86</cell><cell cols="3">11.39 14.50 12.42</cell></row><row><cell cols="4">dependencies in both the input and output, is more accurate (and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">thus, has a lower percentage of mis-labeled keyphrases) showed by</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">darker cells corresponding to the KP-KP entry in all matrices.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison results for document ("doc") vs. sentence ("sent") level for Bi-LSTM-CRF on the kp20k dataset.</figDesc><table><row><cell>Method</cell><cell>Pr%</cell><cell>Re%</cell><cell>F1%</cell></row><row><cell>Bi-LSTM-CRF</cell><cell cols="3">doc sent 64.19 24.66 35.63 67.30 30.32 41.81</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/LiyuanLucasLiu/LM-LSTM-CRF</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/memray/seq2seq-keyphrase</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>We thank <rs type="funder">NSF</rs> for support from grants <rs type="grantNumber">IIS-1802358</rs>, <rs type="grantNumber">IIS-1813571</rs>, <rs type="grantNumber">CNS-1853919</rs>, and <rs type="grantNumber">CNS-1823288</rs>, which supported this research.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_RQMdgfQ">
					<idno type="grant-number">IIS-1802358</idno>
				</org>
				<org type="funding" xml:id="_7fAEeun">
					<idno type="grant-number">IIS-1813571</idno>
				</org>
				<org type="funding" xml:id="_JKre68w">
					<idno type="grant-number">CNS-1853919</idno>
				</org>
				<org type="funding" xml:id="_sJWgYKN">
					<idno type="grant-number">CNS-1823288</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Building a Scientific Concept Hierarchy Database (SCHBase)</title>
		<author>
			<persName><forename type="first">Eytan</forename><surname>Adar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srayan</forename><surname>Datta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-Task Learning of Keyphrase Boundary Classification</title>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="341" to="346" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Using noun se heads to extract document keyphrases</title>
		<author>
			<persName><forename type="first">Ken</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadia</forename><surname>Cornacchia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
			<biblScope unit="page" from="40" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Keyphrase Extraction in Scientific Articles: A Supervised Approach</title>
		<author>
			<persName><forename type="first">Pinaki</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kishorjit</forename><surname>Nongmeikapam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivaji</forename><surname>Bandyopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012: Demonstration Papers</title>
		<meeting>COLING 2012: Demonstration Papers<address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Comparison of Supervised Keyphrase Extraction Models</title>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Florin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cornelia</forename><surname>Bulgarov</surname></persName>
		</author>
		<author>
			<persName><surname>Caragea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web Companion, WWW 2015</title>
		<meeting>the 24th International Conference on World Wide Web Companion, WWW 2015<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-18">2015. May 18-22, 2015</date>
			<biblScope unit="page" from="13" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Citation-Enhanced Keyphrase Extraction from Research Papers: A Supervised Approach</title>
		<author>
			<persName><forename type="first">Cornelia</forename><surname>Caragea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florin</forename><surname>Bulgarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreea</forename><surname>Godea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujatha</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gollapalli</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1435" to="1446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SGRank: Combining Statistical and Graphical Methods to Improve the State of the Art in Unsupervised Keyphrase Extraction</title>
		<author>
			<persName><forename type="first">Soheil</forename><surname>Danesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><surname>Sumner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lexical and Computational Semantics</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page">117</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Kp-miner: Participation in semeval-2</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Samhaa R El-Beltagy</surname></persName>
		</author>
		<author>
			<persName><surname>Rafea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th international workshop on semantic evaluation. Association for Computational Linguistics</title>
		<meeting>the 5th international workshop on semantic evaluation. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="190" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">PositionRank: An Unsupervised Approach to Keyphrase Extraction from Scholarly Documents</title>
		<author>
			<persName><forename type="first">Corina</forename><surname>Florescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cornelia</forename><surname>Caragea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1105" to="1115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain-Specific Keyphrase Extraction</title>
		<author>
			<persName><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><forename type="middle">W</forename><surname>Paynter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Gutwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><forename type="middle">G</forename><surname>Nevill-Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 16th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="668" to="673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Extracting keyphrases from research papers using citation networks</title>
		<author>
			<persName><forename type="first">Sujatha</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gollapalli</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Cornelia</forename><surname>Caragea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th AAAI</title>
		<meeting>the 28th AAAI</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1629" to="1635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Incorporating Expert Knowledge into Keyphrase Extraction</title>
		<author>
			<persName><forename type="first">Sujatha</forename><surname>Das Gollapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoli</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference</title>
		<meeting>the Thirty-First AAAI Conference<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3180" to="3187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, speech and signal processing (icassp), 2013 ieee international conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Extracting key terms from noisy and multitheme documents</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Grineva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Grinev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lizorkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on World Wide Web</title>
		<meeting>the 18th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="661" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Corephrase: Keyphrase extraction for document clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Khaled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><forename type="middle">N</forename><surname>Hammouda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">S</forename><surname>Matute</surname></persName>
		</author>
		<author>
			<persName><surname>Kamel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Data Mining in Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="265" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic Keyphrase Extraction: A Survey of the State of the Art</title>
		<author>
			<persName><forename type="first">Saidul</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd ACL</title>
		<meeting>the 52nd ACL<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1262" to="1273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Bidirectional LSTM-CRF models for sequence tagging</title>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improved automatic keyword extraction given more linguistic knowledge</title>
		<author>
			<persName><forename type="first">Anette</forename><surname>Hulth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Phrasier: A System for Interactive Document Retrieval Using Keyphrases</title>
		<author>
			<persName><forename type="first">Steve</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">S</forename><surname>Staveley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 22nd Annual International ACM SIGIR</title>
		<meeting>of the 22nd Annual International ACM SIGIR</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic hypertext keyphrase detection</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kelleher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saturnino</forename><surname>Luz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1608" to="1609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic keyphrase extraction from scientific articles</title>
		<author>
			<persName><forename type="first">Nam</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olena</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Medelyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language resources and evaluation</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="723" to="742" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando Cn</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised approaches for automatic keyword extraction using meeting transcripts</title>
		<author>
			<persName><forename type="first">Feifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deana</forename><surname>Pennell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="620" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Empower Sequence Labeling with Task-Aware Neural Language Model</title>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic keyphrase extraction via topic decomposition</title>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yabin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="366" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Clustering to find exemplar terms for keyphrase extraction</title>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yabin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">HUMB: Automatic key term extraction from scientific articles in GROBID</title>
		<author>
			<persName><forename type="first">Patrice</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Romary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation. Association for Computational Linguistics</title>
		<meeting>the 5th International Workshop on Semantic Evaluation. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="248" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end Sequence Labeling via Bidirectional LSTM-CNNs-CRF</title>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1101" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sem-Graph: Extracting keyphrases following a novel semantic graph-based approach</title>
		<author>
			<persName><forename type="first">Juan</forename><surname>Martinez-Romo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lourdes</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andres</forename><forename type="middle">Duque</forename><surname>Fernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="82" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Human-competitive tagging using automatic keyphrase extraction</title>
		<author>
			<persName><forename type="first">Eibe</forename><surname>Olena Medelyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep Keyphrase Generation</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Brusilovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="582" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">TextRank: Bringing Order into Text</title>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Černockỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Keyphrase extraction in scientific publications</title>
		<author>
			<persName><forename type="first">Thuy</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Dung</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Digital Libraries</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="317" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Citation Summarization Through Keyphrase Extraction</title>
		<author>
			<persName><surname>Vahed Qazvinian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arzucan</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><surname>Özgür</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd COLING (COLING &apos;10)</title>
		<meeting>the 23rd COLING (COLING &apos;10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="895" to="903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Supervised Keyphrase Extraction as Positive Unlabeled Learning</title>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Sterckx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cornelia</forename><surname>Caragea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Develder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1924">2016. November 1-4, 2016. 1924-1929</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Salience Rank: Efficient Keyphrase Extraction with Topic Modeling</title>
		<author>
			<persName><forename type="first">Nedelina</forename><surname>Teneva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="530" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Single Document Keyphrase Extraction Using Neighborhood Knowledge</title>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 AAAI</title>
		<meeting>the 2008 AAAI</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="855" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Corpus-independent Generic Keyphrase Extraction Using Word Embedding Vectors</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software Engineering Research Conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">KEA: Practical automatic keyphrase extraction</title>
		<author>
			<persName><forename type="first">Gordon</forename><forename type="middle">W</forename><surname>Ian H Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eibe</forename><surname>Paynter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig G Nevill-Manning</forename><surname>Gutwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth ACM conference on Digital libraries</title>
		<meeting>the fourth ACM conference on Digital libraries</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="254" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Automatic Keyword Extraction from Documents Using Conditional Random Fields</title>
		<author>
			<persName><forename type="first">Chengzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Information Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1169" to="1180" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Keyphrase extraction using deep recurrent neural networks on Twitter</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="836" to="845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A comparative study on key phrase extraction methods in automatic web site summarization</title>
		<author>
			<persName><forename type="first">Yongzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Milios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nur</forename><surname>Zincir-Heywood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Digital Information Management</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">323</biblScope>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
