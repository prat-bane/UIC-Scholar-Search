<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Monotonic Location Attention for Length Generalization</title>
				<funder>
					<orgName type="full">UIC Discovery Partners Institute</orgName>
					<orgName type="abbreviated">DPI</orgName>
				</funder>
				<funder ref="#_2zBadkd #_YyBudKS">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jishnu</forename><forename type="middle">Ray</forename><surname>Chowdhury</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Cornelia</forename><surname>Caragea</surname></persName>
						</author>
						<title level="a" type="main">Monotonic Location Attention for Length Generalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B34F4C16F7E22633E9F55E17F161801D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-11-27T03:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We explore different ways to utilize positionbased cross-attention in seq2seq networks to enable length generalization in algorithmic tasks. We show that a simple approach of interpolating the original and reversed encoded representations combined with relative attention allows nearperfect length generalization for both forward and reverse lookup tasks or copy tasks that had been generally hard to tackle. We also devise harder diagnostic tasks where the relative distance of the ideal attention position varies with timestep. In such settings, the simple interpolation trick with relative attention is not sufficient. We introduce novel variants of location attention building on top of <ref type="bibr" target="#b12">Dubois et al. (2020)</ref> to address the new diagnostic tasks. We also show the benefits of our approaches for length generalization in</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Neural seq2seq <ref type="bibr">(Sutskever et al., 2014</ref>) is a powerful generic framework for the task of transforming an input sequence of arbitrary length into an output sequence of arbitrary length. Although seq2seq models can perform impressively in a great variety of tasks <ref type="bibr" target="#b35">(Raffel et al., 2020;</ref><ref type="bibr" target="#b22">Lewis et al., 2020)</ref>, they can still struggle in out-of-distribution generalization (e.g., systematic generalization or length generalization), and sometimes, even in simple algorithmic tasks <ref type="bibr" target="#b20">(Kim et al., 2022;</ref><ref type="bibr" target="#b12">Dubois et al., 2020;</ref><ref type="bibr" target="#b11">Dehghani et al., 2019;</ref><ref type="bibr" target="#b21">Lake &amp; Baroni, 2018;</ref><ref type="bibr" target="#b26">Liska et al., 2018)</ref>. Even after extensive pre-training, neural models can show mixed results in such forms of generalization <ref type="bibr" target="#b20">(Kim et al., 2022;</ref><ref type="bibr" target="#b0">Anil et al., 2022)</ref>.</p><p>In this paper, we focus on length generalization, i.e., the ability of a model to generalize to sequences of unseen (and typically higher) lengths. Particularly, we concentrate on enhancing the interlayer attention mechanism in seq2seq encoder-decoder models for improved length generalization. Similar to <ref type="bibr" target="#b9">Csordás et al. (2022)</ref>, we take a bottom up approach to model development and explore the effects of different strategies of increasing complexities on a range of controlled synthetic probing tasks-each targeting a narrowly defined model behavior or phenomenon-to investigate which strategy works and to what extent, and why does or does not work, and thus, each task precisely pinpointing their capabilities as well as their limitations. Such thorough investigation in a natural language domain can be difficult for at least the following reasons: (1) it can be hard to isolate the exact reasons of failure in natural language due to its complexities and diversity; (2) often there can be exploitable heuristics like emphasis on recency that may improve the overall length generalization performance but preserve systematic issues leading to failures in cases where the heuristics do not apply. Such failures may not be reflected in the overall evaluation if the heuristics apply in the majority of the samples. Besides these factors, the simple synthetic tests that we consider here can be still fairly challenging for neural models. We believe they offer an initial step toward the design of more general-purpose models.</p><p>To achieve the above desideratum and evaluate length generalization capability of different interlayer attention mechanisms, we set up ten synthetic probing task (see Table <ref type="table" target="#tab_1">1</ref> and <ref type="table" target="#tab_2">§2</ref>). Following prior work <ref type="bibr" target="#b16">(Graves et al., 2014;</ref><ref type="bibr" target="#b11">Dehghani et al., 2019;</ref><ref type="bibr" target="#b23">Liang et al., 2021)</ref>, we first consider the task of simply copying source texts in both forward and backward (reverse) directions. Following <ref type="bibr" target="#b12">Dubois et al. (2020)</ref>, we also consider compositional lookup table task <ref type="bibr" target="#b26">(Liska et al., 2018)</ref> in both directions. However, as we will show in §2, in these tasks the ideal attention position can be trivially determined from the decoding timestep alone-a condition (let us call it C1) that simply allows the relative positional attention <ref type="bibr" target="#b37">(Shaw et al., 2018;</ref><ref type="bibr" target="#b10">Dai et al., 2019)</ref> to perform perfectly given the right implementation. Thus, we propose new probing tasks involving repeated copy (ReCopy) and its variants to create settings where C1 is not satisfied. While there are already more synthetic tasks where C1 is not satisfied, our proposed tasks (ReCopy and its variants) are intended to be</p><p>Task Input Output Copy 4 7 9 8 4 7 9 8 Reverse Copy 4 7 9 8 8 9 7 4 Lookup 010 t3 t4 t2 t6 t1 . 010 011 010 011 001 001 Reverse Lookup t1 t6 t2 t4 t3 010 . 010 011 010 011 001 001 ReCopy 4 7 9 8 4 4 4 7 7 7 7 7 9 9 9 9 9 8 8 8 8 8 Reverse ReCopy <ref type="table" target="#tab_8">4 7 9 8  8 8 8 8 8 9 9 9 9 9 7 7 7 7 7 4 4 4  Inv ReCopy  4 4 4 7 7 7 7 7 9 9 9 9 9 8 8 8 8 8 4 7 9 8  Inv Reverse ReCopy 8 8 8 8 8 9 9 9 9 9 7 7 7 7 7 4 4 4 4 7 9</ref>  small extensions over simple copy tasks such that the exact cause of model failure can be clearer compared to more complex tasks. Not only do we propose new probing tasks, but we also propose new strategies to tackle them. Prior models <ref type="bibr" target="#b11">(Dehghani et al., 2019;</ref><ref type="bibr" target="#b12">Dubois et al., 2020)</ref> already struggled in reverse copy or reverse lookup tasks. We introduce a technique of interpolating forward and reversed encoded representations to handle reverse direction even with simple relative attention (the technique is universally applicable to any seq2seq architecture). Moreover, we also propose new attention models, OneStep attention and monotonic location attention (our full model), to handle the proposed probing tasks on which the prior models fail. We also show that our models maintain comparable performance in the SCAN <ref type="bibr" target="#b21">(Lake &amp; Baroni, 2018</ref>) (a dataset for translating simple commands into sequences of actions) and CFQ <ref type="bibr" target="#b19">(Keysers et al., 2020)</ref> length splits (a dataset for query-to-SQL translation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Probing Tasks</head><p>We now describe the ten 2 probing tasks used in this paper.</p><p>We present examples for each task in Table <ref type="table" target="#tab_1">1</ref>.</p><p>Copy: The copy task requires copying input tokens into the output tokens. In this case, the encoder-decoder network has to simply learn an identity function (x = f (x)). For this task we use a vocabulary of natural number tokens from 0-9 (see Table <ref type="table" target="#tab_1">1</ref> for an example). We generated 10, 000 training samples with sequence length in the range 5-10.</p><p>For the development set, we generated 2, 000 samples of sequence length 10-15. For test sets, we generated a split with sequence length 15, another split with sequence length 30, and another with sequence length 100. Each test split has 2, 000 samples.</p><p>Reverse Copy: In the reverse copy task, the model has to copy the input as above but in the reverse direction (see Table <ref type="table" target="#tab_1">1</ref> for an example). This task is generated with the same parameters as the copy task.</p><p>Lookup: Lookup represents the "Long Lookup Tables"</p><p>2 Twelve including tasks in Appendix A.</p><p>task <ref type="bibr" target="#b26">(Liska et al., 2018)</ref> as made available in the code. <ref type="foot" target="#foot_1">3</ref>For any input like "001 t1 t2 t3 .", the output for this task will be "v1 v2 v3 v4" where v1 = 001, v2 = t1(001), v3 = t2(t1(001)), and v4 = t3(t2(t1(001))). Here, t1, t2, and t3 are functions, each corresponding to some lookup table such that t i : {0, 1} 3 → {0, 1} 3 (for any natural number i). The task is generated using the supplied code. 3  The code generates a training split of approximately 9, 000 samples of lengths ≤ 6. We consider three generated test splits that are of sequence lengths 7, 9, and 11. The first test split has approximately 4, 500 samples whereas the others have approximately 5, 000 samples. The development split consists of about 500 samples of sequence length ≤ 6 and approximately 500 samples of length 7.</p><p>Reverse Lookup: Reverse Lookup represents the "Long Lookup Tables Reverse" task <ref type="bibr" target="#b26">(Liska et al., 2018)</ref> as can be generated from the code. 3 For any input like "t1 t2 t3 001 .", the output for this task will be "v1 v2 v3 v4" where v4 = 001, v3 = t3(001), v2 = t2(t3(001)), and v1 = t1(t2(t3(001))). Here, t1, t2, and t3 are lookup functions as before. The splits of this task are created similarly to those of the Lookup task described above.</p><p>ReCopy: There is one thing that is common in the above tasks. For the forward tasks (Lookup, Copy), assuming that the encoder can keep the information of position i at position i after encoding with necessary contextualization (e.g., composition of previous functions in case of Lookup), the ideal encoding position to attend during decoding will always remain at the same constant distance from the decoding timestep. This is also true for the reversed tasks (Reverse Copy, Reverse Lookup) if the encoding is reversed. For example, to copy "4 7 9 8", at timestep 1 the model has to attend position 1 to print 4. Next, at timestep 2 the model has to attend position 2 to print 7. Thus, more generally, for any timestep t the model has to attend an encoding position i such that i -t = c (where c is some constant. In this example, c = 0). Even more generally, in all these tasks, the ideal position to attend can be determined just based on the decoding timestep t. For instance, for the above tasks, the ideal position of attention i can be defined as a function over timestep as i = f (t) = t + c. However, such a happy situation will not be maintained in more complex tasks.</p><p>Thus, we decide to create a new set of diagnostic/probing tasks that are close to the previous tasks but precludes the possibility of determining the position of attention just from the timestep. With this motivation, first, we introduce the task ReCopy (Repeated Copy). In this task, the vocabulary includes natural numbers in the range 0-9. If the input for the task is "4 7 9 8", then the corresponding output will be "4 4 4 7 7 7 7 7 9 9 9 9 9 8 8 8 8 8". Effectively, in this task, the model has to learn to not just copy but also to repeat the copied item for a certain frequency before copying the next item. There is a specific set of rules behind how many times an item should be repeated. That is, if the item is a number ≤ 3 the model should print it once, if the item is a number x such that 3 &lt; x ≤ 6 the model should print it three times, and for any other number &gt; 6, the model should print it five times. The splits and sample sizes for this task are similar to those of the copy task.</p><p>Our intention here is to make a small extension of the copy task that avoids determination of the attention position purely from the timestep but without introducing any additional sources of difficulty so that the causes of failures can be disentangled more easily. For instance, if a model succeeds in the copy task but fail in ReCopy we can now reasonably infer that its cause of failure is the specific difficulty introduced in ReCopy. Note that if we made ReCopy a bit simpler by requiring each number to be copied and repeated for a uniform frequency, then the determination of the ideal position for attention will again become trivially possible just from a decoding timestep; thus ReCopy requires repeating with varying frequency depending on which number is being copied.</p><p>Reverse ReCopy: The Reverse ReCopy task is similar to the ReCopy task in all aspects other than the fact that the copying takes place in the reversed direction (see example in Table <ref type="table" target="#tab_1">1</ref>). The task splits are generated in the same way as in the Copy task.</p><p>Inv ReCopy: The Inv ReCopy task (Inverse ReCopy) is similar to the ReCopy task in all aspects other than the fact that the inputs and outputs are inverted (see example in Table <ref type="table" target="#tab_1">1</ref>). The task splits are generated in the same way as in the Copy task.</p><p>Inv Reverse ReCopy: The Inv Reverse ReCopy task (Inverse Reverse ReCopy) is similar to the Reverse ReCopy task in all aspects other than the fact that the inputs and outputs are inverted (see example in Table <ref type="table" target="#tab_1">1</ref>). The task splits are generated in the same way as in the Copy task.</p><p>SCAN: SCAN is a popular dataset used for testing system-atic generalization <ref type="bibr" target="#b21">(Lake &amp; Baroni, 2018)</ref>. It involves the task for translating simple commands into a sequence of actions. We explore its original length generalization split.</p><p>CFQ: CFQ is a realistic semantic parsing dataset <ref type="bibr" target="#b19">(Keysers et al., 2020)</ref> proposed for evaluating compositional generalization. We explore its length generalization split.</p><p>We also propose and explore two additional probing tasks (DeDupe and PosRetrieve) in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Seq2Seq General Framework</head><p>A seq2seq model can be formalized as a function F seq2seq : N s → N z that maps some input sequence x 1:s = (x 1 , x 2 , . . . , x s ) of length s to an output sequence y 1:z = (y 1 , y 2 , . . . , y z ) of length z. Here each element in x 1:s and y 1:z is a natural number that indexes some distinct token from a vocabulary. F seq2seq constitutes two major components: an encoder function (F enc ) and a decoder function (F dec ). The encoder F enc : N s → IR s×d maps the initial token indices x 1:s to a sequence of hidden state representations e 1:s = (e 1 , e 2 , . . . , e s ) (where any e i ∈ IR d ). The decoder F dec : N * × N → N generates the output sequence y 1:z recursively one token at a time, typically in an autoregressive manner. That is, at each timestep t (beginning at t = 1), F dec takes as input the history of all previously generated tokens H t-1 = (go, y 1 , y 2 , . . . , y t-1 ) and the last generated token index y t and outputs y t+1 . H 0 is initialized with (go) where go represents the index of a special token that marks the beginning of the generation.</p><p>One salient component within the decoder is an interlayer (cross) attention function <ref type="bibr" target="#b1">(Bahdanau et al., 2015)</ref> that allows the decoder to interact and retrieve encoded state information. The decoder, in timestep t, will typically create historycontextualized representation h t-1 ∈ IR d (compressing H t-1 ). Let query q t-1 = f q (h t-1 ), keys k i = f k (e i ), and values v i = f v (e i ) (∀i ∈ {1, . . . , s}) where f q , f k , and f v are linear transformations (f q|k|v : IR d → IR d ). In the attention layer, the query interacts with the keys to score each corresponding value. A weighted sum of values based on those scores is then computed as the result of the attention function. This allows the decoder to dynamically and softly retrieve information from any position in the encoded representations e 1:s at any timestep. For our work, we explore a variety of cross-attention functions which we discuss below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Prior Approaches to Cross-Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Content Attention</head><p>As a baseline, we use the popular scaled inner dot-product query-key based attention as used in <ref type="bibr">Vaswani et al. (2017)</ref>:</p><formula xml:id="formula_0">c ti = &lt; q t , k i &gt; √ d ,<label>(1)</label></formula><formula xml:id="formula_1">a ti = exp(c ti ) s j=1 exp(c tj ) , o t = f o ( s j=1 a tj • v j ),<label>(2)</label></formula><p>where f o : IR d → IR d is a linear transformation, c ti , a ti ∈ IR and o t ∈ IR d . Note that this is a fully content-based attention because it does not explicitly use any position or distance-related information about the query or keys.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Relative Attention</head><p>As another baseline, we use the relative attention mechanism 4 as used in <ref type="bibr" target="#b10">Dai et al. (2019)</ref>. Effectively, a sinusoidal positional encoding <ref type="bibr">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b10">Dai et al., 2019)</ref> is first used to create embeddings for each relative distance.</p><p>Let pe k ∈ IR d represent the embedding for the distance k ∈ Z. Then, the relative position attention creates a querykey score sequence based on the corresponding relative distances between the query and the keys:</p><formula xml:id="formula_2">r ti = &lt; (q t + b 2 ), pe i-t &gt; √ d (3)</formula><p>where b 2 ∈ IR d is a bias for position attention and r ti ∈ IR. This is integrated with content-based attention by modifying Eqn. 1 in §4.1 as:</p><formula xml:id="formula_3">c ti = &lt; (q t + b 1 ), k i &gt; √ d + r ti<label>(4)</label></formula><p>b 1 ∈ IR d is a bias for content-based attention. Everything else is kept the same as was for content-based attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Location Attention</head><p>Location attention, as introduced in <ref type="bibr" target="#b12">Dubois et al. (2020)</ref>, is primarily a form of attention based only on the positions of the encodings e 1:s ; however, it is more expressive than the relative positional attention. Here we discuss the details of location attention with some refinements. <ref type="bibr" target="#b12">Dubois et al. (2020)</ref> introduced a method to determine the locational "center of focus" for attention which is made to resemble human attention in visual search in how even when it focuses on a specific part of the input, it also perceives neighboring parts due to the eccentricity effect <ref type="bibr" target="#b6">(Carrasco et al., 1995)</ref>. Let µ t ∈ IR represent the center of focus such that positions close to µ t get higher attention than those farther away. With such µ t , an attention spread can be modeled by using µ t as a mean in a Gaussian distribution:</p><formula xml:id="formula_4">λ ti = exp - (i -µ t ) 2 2 • σ 2 t (5)</formula><p>Here σ t is the standard deviation, which determines the spread of the attention focus. However, using raw values of i and µ t is not ideal because the range of values (especially 4 Initially the idea was introduced in <ref type="bibr" target="#b37">Shaw et al. (2018)</ref>.</p><p>of i) can differ based on sequence length. This becomes more problematic for unseen length generalization. Thus, the formalism is modified as follows:</p><formula xml:id="formula_5">λ ti = exp - (norm(i) -clamp(µ t )) 2 2 • σ 2 t (6)</formula><p>where:</p><formula xml:id="formula_6">norm(i) = i -1 max(1, s -1) (7) clamp(µ t ) = max(0 + m • µ t , min(1 + m • µ t , µ t )) (8)</formula><p>Note that the encoder position index ranges from 1 to s where s is the sequence length. The norm() function squeezes any position index i into the range [0, 1] no matter the sequence length. Further, the clamp() function enforces µ t to be approximately within [0, 1] which is the possible range of positions that can be attended. Following <ref type="bibr" target="#b12">Dubois et al. (2020)</ref>, m in clamp() acts as a negative slope (m = 0.01) to add some "leakiness" similar to LeakyReLU. Note that the result is a PDF over the whole real number set whereas only the discrete positions of the encoding matter. Thus, λ ti can be further normalized to get a discretized probability measure over only the relevant positions:</p><formula xml:id="formula_7">λ ′ ti = λ ti s j=1 λ tj<label>(9)</label></formula><p>This gives the location attention. Below we discuss how to obtain µ t and σ t . First, a transformation over the decoder hidden state h t is created as l t = f l (h t ) where f l : IR d → IR d is a linear transformation. <ref type="foot" target="#foot_2">5</ref> Next, σ t is computed as:</p><formula xml:id="formula_8">σ t = ReLU (f σt (l t )) + min σt s<label>(10)</label></formula><p>Here f σt : IR d → IR is a linear transform and min σt is the minimum value for σ t . Next, µ t is computed by taking some steps (in either forward or backward direction) with respect to some reference position ref t . Given that the norm(.) function will squeeze any discrete position index into a continuous number in [0, 1], ref t can also be treated to be in [0, 1]. Formally, µ t is computed as:</p><formula xml:id="formula_9">µ t = ref t + stepsize • steps t (11)</formula><p>Here, stepsize is</p><formula xml:id="formula_10">1 max(1,s-1)</formula><p>. For the reference point ref t , different possible choices can be considered. One possible choice is the previous attended position pa t-1 which is computed as pa t-1 = s i=1 α t-1i • norm(i) where α t-1i represents the interlayer attention at the previous timestep (t -1) to the encoding position i. Essentially, with this setup, the attention model can move left or right with respect to previously attended position. Another choice for the reference point is to simply make a neural network-based logistic prediction to choose any arbitrary position b t in [0, 1] as b t = sigmoid(f b (l t )) where f b : IR d → IR is a linear transform. Here b t can also help initialize ref t to adaptively learn to start attending from the beginning of the encoding (i = 1) or the end of the encoding (i = s) (or even somewhere in-between if needed) based on the task. Ultimately, we can allow the model itself to learn to choose or combine both pa t-1 and b t as needed:</p><formula xml:id="formula_11">ref t = g t • pa t-1 + b t (12)</formula><p>with g t being a real scalar in [0, 1] functioning as a gate that decides to keep or ignore pa t-1 . It is computed as</p><formula xml:id="formula_12">g t = sigmoid(f g (l t )) where f g : IR d → IR is a linear transform.</formula><p>Next the steps to take (i.e., steps t ) with respect to the reference point are determined as follows:</p><formula xml:id="formula_13">steps t = softstair(f step (l t ))<label>(13)</label></formula><p>where f step : IR d → IR is again a linear transform and softstair is an activation function that pushes the output towards an integer:</p><formula xml:id="formula_14">softstair(x) = ⌊x⌋ + sigmoid(τ • (x -⌊x⌋ -0.5)) (14)</formula><p>τ is a temperature hyperparameter which is set to 20 like in <ref type="bibr" target="#b12">Dubois et al. (2020)</ref>. Last, the attention is computed as a convex combination of content attention and location attention:</p><formula xml:id="formula_15">a ti = mix ti • exp(c ti ) s j=1 exp(c tj ) + (1 -mix ti ) • λ ′ ti (15) mix ti = sigmoid(βf mix (h t ))<label>(16)</label></formula><p>Here f mix : IR d → IR is a linear transform and c ti corresponds to the content-based pre-normalized attention scores as computed in Eqn. 1. In some cases, we might want to ignore the content attention focusing purely on location attention. In such cases, we can set a ti = λ ′ ti .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Proposed Approaches to Cross-Attention</head><p>In this section, we first present the limitations of the prior approaches discussed above and then present (in a bottomup manner) our proposed changes that address them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Limitations of Prior Approaches</head><p>Limitation 1 (handling reverse tasks): As noted earlier (see ReCopy task description in §2), in some tasks like Copy or Lookup, the target cross-attention position is always at the same constant relative distance from the timestep. In such cases, the inductive bias from the relative attention ( §4.2) can be especially fruitful. However, this setup is not maintained by default (without reversing the encoding or the input in the model), in the reverse directions of the tasks (Reverse Copy or Reverse Lookup). Consider transforming "4 7 9 8" to "8 9 7 4". In this case to print 8 in timestep t = 1, the model needs to attend to encoding position i = 4. Thus, the relative distance will be i -t = 3. However, for printing 9 in timestep t = 2, the model needs to attend to the encoding position i = 3. Then the relative distance will be i -t = 1. Thus, the ideal relative distance can vary with timestep and also depends on the source sequence length. These facts make it a struggle for relative attention, by default, to work on reverse tasks. In theory, location attention is equipped to handle reverse tasks -it has to just initialize b t as 1 and g t as 0 when t = 1. This will set ref t = 1, i.e., the reference position will be the end of the input sequence.</p><p>From that point location attention can learn to take steps backward one by one using previous attention (pa t-1 ) as the reference position if needed. However, in practice, location attention still tends to be empirically brittle and have been shown to fail the reverse lookup task <ref type="bibr" target="#b12">(Dubois et al., 2020)</ref>.</p><p>Limitation 2 (handling ReCopy and beyond): As discussed in §2 (see ReCopy description), tasks like ReCopy, Reverse ReCopy, or their inverted variants are specifically designed to serve as settings in which the ideal attention position can vary from timestep to timestep (no matter if the encoding positions are reversed or not). Thus, this setting becomes hard for relative attention. Location attention, again, can theoretically address these situations given its flexibility to keep track of past attended position and ability to take any arbitrary steps in reference to past attended position dependent on the decoder state. Nevertheless, as mentioned earlier, in practice location attention turns out to be relatively brittle. Moreover, its use of soft sigmoid-based gating for making decisions at different stages of the model can lead to higher error accumulation and lower robustness to increasing lengths of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Bidirectional Relative Attention</head><p>First, we propose a simple approach to extend relative attention in a manner that addresses limitation 1. We note that if the task is, e.g., reverse copy, we can simply reverse the encoded sequence representations after encoding. Once done so, from the perspective of the decoder, the task becomes equivalent to forward copy. Nevertheless, in practice, we will not know ahead of time whether we are facing a task where forward version of the encoding is more ideal or the reversed version of the encoding. Thus, we use a gating mechanism that interpolates (make a convex combination of) the two directions of encoding so that the model can adaptively decide whether to reverse the encodings or not:</p><p>e rev 1:s = reverse(e 1:s ),</p><formula xml:id="formula_16">α dir = sigmoid(β•f dir (e cls )) (17) ∀i ∈ {1, . . . , s} e dir i = α dir • e i + (1 -α dir ) • e rev i (<label>18</label></formula><formula xml:id="formula_17">)</formula><p>β is a scalar (acting as a temperature), f dir : IR d → IR 1 is a linear layer, and e cls ∈ IR d is a vector representation of the whole sequence e 1:s -it can be implemented in multiple ways (we explain our implementation in Appendix E). After this we use the same strategy as in §4.2 but using key and value transformations over e dir instead of e. This trick can also be useful in more practical tasks like translation where the source and target language may have different reading orders. Note that e dir is different from outputs from models like bidirectional RNNs. Unlike here, in a bidirectional RNN, the encoded tokens remain in the same positions; only the contextualizing information comes from different directions. Also, note that this strategy is as general purpose as introducing bidrectionality to RNNs. Moreover, we are allowing neural networks to dynamically predict the direction for a given input through the gating mechanism; thus, avoiding infusion of task-specific knowledge of ideal direction of attention. 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">OneStep Attention</head><p>As discussed in §5.1, fixing limitation 1 by reversing the encodings (as in §5.2) still does not address limitation 2. Concerned with limitation 2, we move away from simple relative positional attention and instead seek to make adjustments over location attention to address its potential issues (see §5. First Change: The first change follows from §5.2 and is motivated to address limitation 1.</p><p>Second Change: The second change is motivated by a number of factors. First, due to the incorporation of the first change, the role of b t from Eqn. 12 is severely undermined. It is not anymore necessary for b t to initialize the starting position of attention to handle reverse tasks. Besides that the usefulness of b t can be limited. 7 It is motivated for percentile attention in <ref type="bibr" target="#b12">Dubois et al. (2020)</ref> which may not 6 While this strategy may appear obvious, it is still not explored so far to our knowledge. Moreover, theoretical motivation does not always translate well to empirical performance. For instance, Location Attention struggles in reverse tasks despite having the theoretical capacity for reverse attention as discussed in §5.1. So empirical benefit of this strategy is not a priori obvious and deserves the investigation that we do here. 7 It can be still useful in special cases when the model has to attend some x position from the end in one timestep and some y position from the beginning in another. be as relevant or can be accomodated by content attention mixing (Eqn. 15). So we removed it. To reduce erroraccumulation we also remove the gating g t over pa t-1 ; thus ultimately setting ref t = pa t-1 . It removes the models capacity for attending to some specific absolute position from the beginning/end but this capacity is also lacking from relative attention and is not currently required by most of our tasks. We keep investigation to incorporating this capacity better in the future. Currently, absolute positional encoding in the encoder combined with content attention mixing can still accommodate for the lack to an extent.</p><p>Third Change: In the third change, we replace softstair with a sigmoid for the step computation. The sigmoid function enforces the model to softly choose between either taking a single step forward (steps t = 1) or none (steps t = 0). We added this change because giving unrestricted freedom in determining the steps can make it harder for the model to learn the right function. Particularly in most of our current diagnostic tasks, it is sufficient to learn to make bounded steps in [0, 1] with respect to the past attended position. While this choice is perhaps not ultimately ideal, it helps us evaluate the breaking points of the Location Attention framework better. Regardless, even after this restriction, On-eStep can be still powerful enough to simulate a windowed version of relative attention (if it takes a single step in every timestep) <ref type="bibr" target="#b37">(Shaw et al., 2018)</ref>. Moreover, a sufficiently powerful encoded representation can, in theory, always reorganize or permute the input information to accommodate for this restriction. Besides, content attention mixing (Eqn. 15) can break the monotonicity of OneStep<ref type="foot" target="#foot_3">foot_3</ref> and make it more flexible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Monotonic Attention</head><p>In some tasks, it can be easier to learn to take bigger steps at the level of interlayer attention instead of expecting the encoder to permute the source input appropriately. So, we create another attention function where we relax the constraints in OneStep by changing the steps computation as:</p><formula xml:id="formula_18">steps t = g • sigmoid(f step (l t )) + (1 -g) • ReLU (f step (l t ))<label>(19</label></formula><p>) Here, g = sigmoid(p) where p ∈ IR is a model parameter. <ref type="foot" target="#foot_4">9</ref>As we can see, with this setup we can allow the model itself to learn to prefer either taking controlled steps with a sigmoid or possibly bigger steps with a ReLU. We still use ReLU activation to keep the attention monotonic (i.e., the attention mechanism can only make forward steps) similar to OneStep for reasons discussed in § §5.3 (in Third Change). Table <ref type="table">3</ref>. Accuracy on SCAN length split and CFQ length split. We report the mean and standard deviation of 5 runs for SCAN and of 3 runs for CFQ. We bold the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Setup</head><p>Similar to <ref type="bibr" target="#b12">Dubois et al. (2020)</ref>, we use a Bidirectional GRU <ref type="bibr" target="#b8">(Chung et al., 2014)</ref> based seq2seq model as the base for all the attention mechanisms. We explain more architectural details and hyperparameters in the Appendix E.</p><p>Nomenclature: In Tables <ref type="table" target="#tab_2">2</ref> and <ref type="table">3</ref>, we use the term Content to refer to content attention ( §4.1), Relative to refer to relative attention ( §4.2), and Bi-Relative for bi-directional relative attention ( §5.2). We use the terms LocAttn, OneStepAttn, and MonoAttn for location attention ( §4.3), OneStep Attention ( §5.3), and monotonic attention ( §5.4) respectively if they are used without mixing content attention (i.e., replacing Eqn. 15 with a ti = λ ′ ti ). Otherwise, we use the terms Mix LocAttn, Mix OneStepAttn, and Mix MonoAttn when mixing with content attention is done (i.e., Eqn. 15 is kept as described). We generally use the unmixed variants on the simpler diagnostic tasks (Lookup, Copy, or ReCopy-based tasks) because position-based attention is what is mainly relevant for the tasks.</p><p>Evaluation: We calculate the sequence-level accuracy of our models. Any generated output gets a score of 1 if and only if it matches exactly with the given target output.</p><p>On the EOS problem: The EOS token is a special marker that a model needs to generate to signify the end of sequence. In similar contexts, some prior works have tried to make the evaluation less stringent <ref type="bibr" target="#b12">(Dubois et al., 2020;</ref><ref type="bibr" target="#b31">Newman et al., 2020</ref>) by terminating the model generation based on the oracle EOS position or by truncating oracle sequence based on predicted EOS position. We do not modify the evaluation in any such non-standard manner. Generally, we do not find EOS prediction to be a problem. If the inductive bias is suitable for the task, our models learn to generalize near perfectly without us needing to incorporate any separate mechanism to predict EOS properly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experimental Results</head><p>In Table <ref type="table" target="#tab_2">2</ref> we show the results of our different attention strategies on all our diagnostic tasks except SCAN and CFQ. The results are close to what we would expect a priori. Pure content attention (Content) without more explicit guidance from any positional information suffers in all the tasks. Relative attention (Relative) does well in the forward copy and lookup tasks but it fails in the reversed tasks for the reasons discussed in §5.2. It also fails in the ReCopy-based tasks. This is consistent with our discussed limitations of prior works in §5.1. Also, consistent with this discussion, we find our implementation of location attention (LocAttn) to struggle in all the tasks. both forward and reverse directions of copy and lookup tasks. This is aligned with our motivation for designing it ( §5.2). However, Bidirectional relative attention still does not alleviate the second limitation ( §5.1) and thus, fail in the ReCopy-based tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bidirectional relative attention (Bi-Relative) succeeds on</head><p>OneStep attention (OneStepAttn) succeeds nearly on all tasks except the inverted variations of the ReCopy tasks. The Copy tasks and Lookup tasks are easy to learn for OneStep attention because in either tasks it has to simply learn to take one step forward relative to the past attended position in every timestep. The ReCopy and Reverse ReCopy is slightly more complicated but still not too hard to learn. In these cases, the model has to learn to wait (predict steps t = 0) while the decoder is repeating previous generations. The attention model has to then predict steps t = 1 to move one step forward in the encoding positions after the repetition of the content from the past attended position is complete. Thus, the OneStep strategy is suitable for the ReCopy and Reverse ReCopy tasks as well.</p><p>However, the OneStep strategy faces an issue for the inverted versions of the tasks. Consider an Inv ReCopy sample where the input is "4 4 4 7 7 7 7 7 9 9 9 9 9 8 8 8 8 8" and the output is "4 7 9 8". In this case, one way to solve this would be for the encoder to radically re-organize the positions of the input information. But if the encoder fails to do that and keeps the encoded information close to its original position, OneStep attention, by itself, is ill-equipped for the task. In the given example, after printing 4 from encoding position 1, in the next timestep it has to take not just one but three steps forward. OneStep attention cannot do that because its number of steps is constrained by a sigmoid.</p><p>In contrast to OneStep attention, monotonic attention (MonoAttn) is more flexible allowing bigger steps when needed. As such, monotonic attention is able to solve Inv ReCopy tasks that OneStep could not. It also performs perfectly on copy tasks and ReCopy tasks in both direc-tions. However, it fails on the lookup tasks. It seems that its increased flexibility (loosened inductive bias) and its possibility to make more uncontrolled steps (which are unnecessary for the lookup tasks) also at the same time make it more confused when trying to learn the lookup tasks in a length-generalizing manner.</p><p>Ultimately, both OneStep attention and monotonic attention perform better than any of the other attention models. Both solves 6 out of the 8 tasks in Table <ref type="table" target="#tab_2">2</ref> with 100% accuracy. However, we also discover a trade-off -the restricted steps of OneStep attention preclude it from solving the inverted versions of ReCopy tasks whereas the more unconstrained steps of monotonic attention manages the inverted ReCopy tasks but at the cost of the lookup tasks.</p><p>In Table <ref type="table">3</ref>, we present the results on SCAN. We find location attention and our extensions of it (OneStep attention or monotonic attention) to generally also perform better on the task of translating simple commands into sequences of actions than other forms of interlayer attention even though they are not designed explicitly keeping the structure of SCAN task in mind. OneStep attention (Mix OneStepAttn) does particularly better than the others in SCAN. In the same table, we also present the results on CFQ. Interestingly, the basic position-encoding-less version of inter-layer attention does the best here. However, both OneStep and monotonic attention keep up with it better than others -like location attention or unidirectional relative attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Additional Analyses</head><p>Ablations: In Table <ref type="table" target="#tab_3">4</ref>, we show some of the main ablations of OneStep Attention. -Step 2 represents using the more sophisticated location attention variant of ref t computation (Eqn. 12) instead of the proposed ref t = pa t-1 change in step 2 in §5.3. -Step 3 represents using softstair activation for step computation (Eqn. 13) from location attention instead of the proposed sigmoid activation in step 3 change of OneStep ( §5.3). -Sigmoid represents removing the activation function from Eqn. 13 altogether. As the ablation shows both of our proposed changes are important to succeed in most of the tasks. Interestingly, we find here that having no activation at all in Eqn. 13 generally serves us better than having softstair activation. Besides that, the ablation results support our original motivation for proposing Step 2 and</p><p>Step 3 in OneStep attention. We show several more ablation tests in Appendix B.</p><p>Additional Tasks: In Appendix A, we introduce and explore two additional tasks -DeDupe and PosRetrieve.</p><p>Alternate Evaluation: In Appendix C we evaluate the models on edit distance instead of exact match accuracy.</p><p>Edit distance serves as a more fine-grained evaluation.</p><p>Examples: In Appendix D we present some example failure cases of OneStep attention and monotonic attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Limitations</head><p>First, although OneStepAttn and MonoAttn perform better than LocAttn in general, they are also more restricted. Nevertheless, OneStepAttn and MonoAttn show the potential of the LocAttn-like framework with restrained room for error accumulation and slightly stronger inductive biases. Ideally, we want to improve upon them in the future to get both higher flexibility and good performance. Moreover, when building specific modelling capacities (say attending to absolute positions), we should also consider building appropriate synthetic tasks for sanity checking in a similar spirit as done in this paper. In Appendix A, we propose Pos-Retrieve which can be a sanity check for absolute position attention capability for future developments.</p><p>Second, our experiments are limited to mainly synthetic tasks most of which require purely location-based attention<ref type="foot" target="#foot_5">foot_5</ref> but no complex synergy between content-based attention and position-based attention. More synthetic tasks for sanity checking such capacities can be built.</p><p>Third, our exploration is currently limited to RNN-based seq2seq models. One reason for focusing on RNNs is because vanilla non-pretrained Transformers encoders can struggle to solve tasks like lookup table for decoder to do its job without specific changes <ref type="bibr" target="#b9">(Csordás et al., 2022)</ref>. Moreover, integration of location attention into Transformers is complicated by the fact that they use multiple layers of crossattention in each timestep introducing additional variables to consider (the problem is not that our methods cannot be integrated with Transformers but that there are many ways to do so). Given these added variables, we leave investigations with Transformers for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusion</head><p>We introduce several new probing tasks -ReCopy and its variants (some others in Appendix A) to enable additional diagnoses of length generalization performance of neural models. Although our proposed tasks are simple, this very simplicity can allow better isolation of failure cases and provide sanity checks for locational reasoning skills. Moreover, the new tasks are still challenging enough that none of the models explored here succeed in all of them.</p><p>We propose a way to softly switch between the forward encodings and its reversed version to get near perfect performance in reverse variants of copy and lookup tasks that have been previously challenging to solve. We illuminate the limits of location attention and show how certain modifications in the form of OneStep attention and monotonic attention can bring massive improvement. Although, the modifications bring stronger inductive biases than location attention, they can still simulate windowed relative attention and empirically demonstrate more stable performance across datasets including more realistic ones like CFQ.</p><p>Monotonic attention or OneStep attention can also be more broadly applicable in any context requiring list traversal i.e. monotonic traversal through a list of items in a backpropagation-friendly manner -for example, one application can be skill selection with a dynamic time horizon instead of a fixed one <ref type="bibr" target="#b14">(Garg et al., 2022)</ref>. OneStep attention is suitable if the only relevant choice during the traversal is to either stay at a position or move to the next position by a single step. Monotonic attention is suitable if we also want to allow the model to skip positions during traversal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Tasks</head><p>We introduce and analyze two additional tasks here. We present examples for each in Table <ref type="table" target="#tab_5">5</ref>.</p><p>DeDupe: As the name suggests, DeDupe is essentially a de-duplication task of removing contiguous repetitions 11 as shown in Table <ref type="table" target="#tab_5">5</ref>. The task is very similar to Inv ReCopy but with a few differences. In Inv ReCopy, repetition occurs based on a fixed rule, each number will be repeated some fixed number of times dependent on that specific number. The task in Inv ReCopy is to remove the fixed number of number-specific repetitions rather than all contiguous repetitions. For instance, if the input is "4 4 4 4 4 4" the output for Inv ReCopy will be "4 4" (because "4" is bound to repeat three times according to the rules of ReCopy) but for the same input, the output for DeDupe will be just "4". Unlike Inv ReCopy, the DeDupe function is not invertible.</p><p>Thus the task cannot be inverted (whereas ReCopy is the inversion of Inv ReCopy). We generate the splits for this task (DeDupe) in the same way as the Copy task.</p><p>PosRetrieve: The task of PosRetrieve is to treat the input values as position indices to retrieve from. Given an input x in a list format, x = [i, j], the output y will be y = i : f (i, x); j : f (j, x);. Here,</p><formula xml:id="formula_19">f (i, x) = x[i] if len(x) &gt; i else f (i, x) = n/a.</formula><p>We generate the splits for this task in the same way as the Copy task. This task can more explicitly check for a models ability to choose some p th position item from the beginning. This was one ability for which location attention was motivated <ref type="bibr" target="#b12">(Dubois et al., 2020)</ref>, but there was no benchmark to explicitly check for this.</p><p>We mainly focus on the forward variations of the task here but reverse variants can also be created.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results:</head><p>In Table <ref type="table">6</ref>, we show the results of the main models on DeDupe and PosRetrieve. Only MonoAttn performs decently in DeDupe which makes sense given that it is the only approach that does well in Inv ReCopy which is similar to DeDupe. DeDupe is, however, a bit harder than Inv ReCopy for MonoAttn because the encoder needs to encode information about total contiguous repetitions from the context so that MonoAttn can predict the right amount of steps without looking ahead (which it cannot without some kind of multistep attention). In Inv ReCopy, the encoder does not have to encode any contextual information, since repetition happens according to a fixed context-independent rule. Thus, we see reduced performance in DeDupe compared to that in Inv ReCopy. None of the models is currently able to do well at PosRetrieve. This is, perhaps, expected for most models since they are currently lacking any explicit capability for modelling absolute positions but Location Attention still struggles with it despite theoretically having the capacity. We leave this task as an open challenge.</p><p>11 We thank one of our reviewers for this idea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Ablation Models and Other Alternatives</head><p>We also show experimental results of different potential alternatives in the vicinity of location attentions and ablations of monotonic attention. We discuss the different models below.</p><p>Bi-ROPE: Here, we use the same strategy as in §5.2 to create the encoder representations but then we apply a different positional encoding for modeling relative distancesrotary positional encodings (ROPE) <ref type="bibr" target="#b38">(Su et al., 2021)</ref>. ROPE rotates the query and key vectors in space based on their sinusoidally encoded positions before using the query and key in a content-based attention as in §4.1.</p><p>LocAttn S: This is a simplified (S) version of location attention (without content attention mixing) where we set ref t = b t for the first timestep to initialize the reference position using b t and then use ref t = pa t-1 like OneStep Attention/monotonic attention. This approach can be thought to be "in between" the original location attention and monotonic attention.</p><p>Mix LocAttn S: This is same as LocAttn S but with content attention mixing (Eqn. 15). Mix LocAttn S PR: When mixing with content attention (Eqn. 15), there is an option to set pa t-1 to track only the location-based attended position to keep as a reference point by setting pa t-1 = s i=1 λ ′ t-1i • norm(i) where λ ′ t-1i is the location-only attention from the past timestep. We use the modifier PR (Position Attention based Reference) to denote this way of setting pa t-1 . Mix LocAttn S PR extends Mix LocAttn S with PR. Mix OneStepAttn PR: This extends Mix OneStepAttn with PR. Mix MonoAttn PR: This extends Mix MonoAttn with PR.</p><p>RMonoAttn: In RMonoAttn (Relaxed Monotonic Attention) we remove the sigmoid completely and overall simplify the step computation to: steps t = ReLU (f step (l t )).</p><p>Mix RMonoAttn: This is RMonoAttn with content attention mixing (Eqn. 15).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mix MonoAttn PR:</head><p>This extends Mix RMonoAttn with PR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Ablation Results</head><p>In Table <ref type="table" target="#tab_8">7</ref>, we show the results of the above models in all the main paper tasks but SCAN and CFQ. Bi-ROPE performs similarly to Bi-Relative as we would expect. LocAttn S with its simplification performs better than LocAttn (Table <ref type="table" target="#tab_2">2</ref>) but still falls behind OneStep/monotonic attention. The Mix variant models tend to perform worse than the unmixed Task Input Output DeDupe 4 4 4 7 7 7 7 9 9 9 9 9 8 8 8 8 8 4 7 9 8 PosRetrieve 5 4 2 7 9 6 9 5 7 3 5:6; 4:9; 2:2; 7:5; 9:3; 6:9; 9:3; 5:6; 7:5; 3:7; Table 6. Accuracy of the models on different length generalization splits in DeDupe and PosRetrieve. We present the median of five runs on different seeds. We bold the best results. .</p><p>ones -this is because these tasks can be done purely based on positional reasoning and the content attention is more likely to confuse the models than help in these specific tasks. PR can help better track past locationally attended positions and thus improve the performance of the Mix models (compared to when they are used without PR). RMonoAttn tends to struggle more compared to MonoAttn demonstrating the value of gating with sigmoid-activated step prediction (eqn. 19).</p><p>In Table <ref type="table">8</ref>, we show the results of the above models for SCAN. In SCAN, the trend reverses a bit -mix models tend to be here better than unmixed ones. We suspect that mixing with content attention is more beneficial in more sophisticated tasks (SCAN is at least relatively more sophisticated than others besides CFQ) because it adds more flexibility. PR can sometimes further help in SCAN too in some models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Alternate Evaluations</head><p>In Table <ref type="table">9</ref>, we show the results of the main models on the probing tasks with mean edit distance 12 as the evaluation metric. This paints a more fine-grained picture of the differences between model performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Examples</head><p>In Table <ref type="table" target="#tab_1">10</ref>, we show some failure case examples from OneStep attention and monotonic attention. Generally we find that they can generate the sequence correctly from the beginning up to a point (the correct generated part is highlighted in cyan) after which things go awry.</p><p>12 Computed using NLTK <ref type="bibr" target="#b5">(Bird et al., 2009)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Experimental Details</head><p>E.1. Architecture For the encoder, a stack of bi-directional GRUs is used. e cls is the concatenation of the final hidden state of the forward encoder GRU and the first hidden state of the backward encoder GRU. In every timestep t, the decoder state h t-1 (h 0 initialized with e cls ) is used as a query and the encoded representations (e 1:s or e dir 1:s depending on need) are used as keys and values. The values are passed through an extra non-linear transformation with LeakyRELU following the code from <ref type="bibr" target="#b12">(Dubois et al., 2020)</ref> before the standard linear transformation. For the non-linear transformation we use the same d neurons per layer. The output of the attention is concatenated with the embedding of the last generated token (initially some special token "go" indicating start of sequence). The concatenation is used as input to a decoder stack of GRUs with h t-1 as the hidden memory. The output of decoder GRU is h t . A linear transformation over h t is used to change the size of the vector from d (size of h t ) to d e (where d e is the embedding size). We then use the transpose of the embedding matrix to create the distribution over vocabulary. We select the maximum scoring token as the generated token for the timestep t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Hyperparameters</head><p>We attempted to keep the hyperparameters similar to <ref type="bibr" target="#b12">(Dubois et al., 2020)</ref>. We use 64 as the embedding size (i.e d e = 64) and single-layered GRUs for the encoder/decoder. The total hidden size for the encoder/decoder GRU is 128 (therefore d = 128). We only use one head for the attention mechanism. We use a dropout of 50% on the encodings similar to <ref type="bibr" target="#b12">Dubois et al. (2020)</ref>. We set β = 5. Following tradition we keep attention head dimension as d/heads which in our case is 128 since heads = 1. For CFQ, we use two layered GRUs for the encoder/decoder and twice Table 8. Accuracy on SCAN length split. We report the mean and std of 5 runs. We bold the best results.</p><p>the hidden size/embedding size than above. Generally, we use a batch size 32, a learning rate of 1e -3 with Adam (default parameters) and no weight decay. We halve the learning rate if the accuracy plateaus for four contiguous epochs. We run the models for a maximum of 100 epochs with 50 patience for early stopping. More hyperparameter details can be found in the codebase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Connections to Other Models</head><p>Neural Turing Machines <ref type="bibr" target="#b16">(Graves et al., 2014)</ref> include one of the earliest incorporation and motivation of a location-based attention (distinguished from content based attention) within a modern neural network-based paradigm. <ref type="bibr" target="#b30">Luong et al. (2015)</ref> proposed a Gaussian distribution-based attention for localized focus which is similar to how eccentricity effect is modeled here. Location attention from <ref type="bibr" target="#b12">Dubois et al. (2020)</ref> expands upon many of these prior ideas.</p><p>In the context of Transformers, countless works proposed ways to include some form of position-based attention bias <ref type="bibr" target="#b37">(Shaw et al., 2018;</ref><ref type="bibr" target="#b48">Yang et al., 2018;</ref><ref type="bibr" target="#b10">Dai et al., 2019;</ref><ref type="bibr" target="#b44">Wang et al., 2020;</ref><ref type="bibr" target="#b18">Ke et al., 2021;</ref><ref type="bibr" target="#b38">Su et al., 2021;</ref><ref type="bibr">Luo et al., 2021;</ref><ref type="bibr" target="#b34">Qu et al., 2021;</ref><ref type="bibr" target="#b7">Chang et al., 2021;</ref><ref type="bibr" target="#b46">Wu et al., 2021;</ref><ref type="bibr" target="#b45">Wennberg &amp; Henter, 2021;</ref><ref type="bibr">Likhomanenko et al., 2021;</ref><ref type="bibr" target="#b13">Dufter et al., 2022;</ref><ref type="bibr" target="#b29">Luo et al., 2022;</ref><ref type="bibr">Sun et al., 2022) (interalia)</ref>. Dynamic convolution <ref type="bibr" target="#b47">(Wu et al., 2019)</ref> and other similar models can also be treated as forms of location attention (query-to-distance-based attention within a local level of encoding (which can be already done with a Bidirectional RNN as shown in the same paper) where it only has to learn to compute and output the final function output. It does not tackle the challenge of doing the task at a seq2seq level (or in the style of a language model) which requires printing a sequence of intermediate function outputs in a rule-based manner in addition to the final output.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>1). As a result we propose a new attention model -OneStep attention. Below we enumerate the main adjustments over location attention (from §4.3): 1. OneStep attends to key-value transformations of e dir 1:s instead of e 1:s similar to §5.2. 2. The computation of ref t is simplified as: ref t = pa t-1 3. The activation function in Eqn. 13 to sigmoid from softstair: steps t = sigmoid(f step (l t ))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Input-output examples for each task (except CFQ).</figDesc><table><row><cell>8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Accuracy of the models on different length generalization splits in different algorithmic diagnostic / probing tasks. We present the median of five runs on different seeds. We bold the best results.</figDesc><table><row><cell>Model</cell><cell></cell><cell>Copy</cell><cell></cell><cell cols="3">Reverse Copy</cell><cell></cell><cell>Lookup</cell><cell></cell><cell cols="3">Reverse Lookup</cell></row><row><cell cols="2">(Length Splits) 15</cell><cell>30</cell><cell>100</cell><cell>15</cell><cell>30</cell><cell>100</cell><cell>7</cell><cell>9</cell><cell>11</cell><cell>7</cell><cell>9</cell><cell>11</cell></row><row><cell>Content</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="2">33.3 0</cell><cell>0</cell><cell>3.7</cell><cell>0</cell><cell>0</cell></row><row><cell>Relative</cell><cell cols="4">100 100 100 0</cell><cell>0</cell><cell>0</cell><cell cols="3">100 100 100</cell><cell cols="2">78.2 0.8</cell><cell>0.4</cell></row><row><cell>LocAttn</cell><cell cols="2">99.8 0</cell><cell>0</cell><cell>0.7</cell><cell>0</cell><cell>0</cell><cell cols="2">100 9.4</cell><cell>0</cell><cell cols="2">13.3 0</cell><cell>0</cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bi-Relative</cell><cell cols="9">100 100 100 100 100 100 100 100 100</cell><cell cols="2">100 100</cell><cell>100</cell></row><row><cell>OneStepAttn</cell><cell cols="9">100 100 100 100 100 100 100 100 100</cell><cell cols="2">100 100</cell><cell>100</cell></row><row><cell>MonoAttn</cell><cell cols="8">100 100 100 100 100 100 100 98</cell><cell>29.9</cell><cell cols="2">28.5 0</cell><cell>0</cell></row><row><cell>Model</cell><cell></cell><cell>ReCopy</cell><cell></cell><cell cols="3">Reverse ReCopy</cell><cell></cell><cell cols="2">Inv ReCopy</cell><cell cols="3">Inv Reverse ReCopy</cell></row><row><cell cols="2">(Length Splits) 15</cell><cell>30</cell><cell>100</cell><cell>15</cell><cell>30</cell><cell>100</cell><cell>15</cell><cell>30</cell><cell>100</cell><cell>15</cell><cell>30</cell><cell>100</cell></row><row><cell>Content</cell><cell cols="2">19.1 0</cell><cell>0</cell><cell>25</cell><cell>0</cell><cell>0</cell><cell cols="2">0.05 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Relative</cell><cell cols="2">43.1 0</cell><cell>0</cell><cell>0.1</cell><cell>0</cell><cell>0</cell><cell cols="2">75.9 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>LocAttn</cell><cell cols="2">79.6 0</cell><cell>0</cell><cell cols="2">19.7 0</cell><cell>0</cell><cell cols="3">99.4 58.8 0</cell><cell cols="2">97.9 0.3</cell><cell>0</cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bi-Relative</cell><cell cols="2">33.4 0</cell><cell>0</cell><cell cols="2">35.3 0</cell><cell>0</cell><cell cols="2">69.8 0</cell><cell>0</cell><cell cols="2">71.3 0</cell><cell>0</cell></row><row><cell>OneStepAttn</cell><cell cols="7">100 100 100 100 100 100 0.1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>MonoAttn</cell><cell cols="12">100 100 100 100 100 100 100 100 98.8 100 99.9 98.3</cell></row><row><cell>Model</cell><cell cols="2">SCAN (Len.)</cell><cell cols="2">CFQ (Len.)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Content</cell><cell cols="2">17.61 ± 4.07</cell><cell cols="2">62, 14 ± 0.88</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Relative</cell><cell cols="2">19.21 ± 5.52</cell><cell cols="2">56.64 ± 1.84</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mix LocAttn</cell><cell cols="2">20.74 ± 5.69</cell><cell cols="2">44.83 ± 9.45</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bi-Relative</cell><cell cols="2">8.41 ± 1.21</cell><cell cols="2">59.48 ± 1.54</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Mix OneStepAttn 29.51 ± 9.46 60.65 ± 3.74</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mix MonoAttn</cell><cell cols="2">21.08 ± 7.17</cell><cell cols="2">60.32 ± 3.58</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Accuracy of ablations over OneStepAttn in different length generalization splits in different algorithmic diagnostic/probing tasks. We present the median of five runs on different seeds. We bold the best results.</figDesc><table><row><cell>Model</cell><cell></cell><cell>Copy</cell><cell></cell><cell></cell><cell cols="2">Reverse Copy</cell><cell></cell><cell>Lookup</cell><cell></cell><cell cols="3">Reverse Lookup</cell></row><row><cell cols="2">(Length Splits) 15</cell><cell>30</cell><cell>100</cell><cell>15</cell><cell>30</cell><cell>100</cell><cell>7</cell><cell>9</cell><cell>11</cell><cell>7</cell><cell>9</cell><cell>11</cell></row><row><cell>OneStepAttn</cell><cell cols="7">100 100 100 100 100 100 100</cell><cell cols="3">100 100 100</cell><cell cols="2">100 100</cell></row><row><cell>-Step 2</cell><cell cols="2">100 1.4</cell><cell>0</cell><cell cols="3">100 98.6 0</cell><cell>99.2</cell><cell cols="2">2.34 0</cell><cell>99.8</cell><cell>0</cell><cell>0</cell></row><row><cell>-Step 3</cell><cell>6.9</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>41.9</cell><cell>0</cell><cell>0</cell><cell>22.9</cell><cell>0</cell><cell>0</cell></row><row><cell>-Sigmoid</cell><cell cols="7">100 100 99.8 100 100 100 100</cell><cell cols="2">74.3 0.3</cell><cell>19.1</cell><cell>0</cell><cell>0</cell></row><row><cell>Model</cell><cell></cell><cell>ReCopy</cell><cell></cell><cell cols="3">Reverse ReCopy</cell><cell cols="3">Inv ReCopy</cell><cell cols="3">Inv Reverse ReCopy</cell></row><row><cell cols="2">(Length Splits) 15</cell><cell>30</cell><cell>100</cell><cell>15</cell><cell>30</cell><cell>100</cell><cell>15</cell><cell>30</cell><cell>100</cell><cell>15</cell><cell>30</cell><cell>100</cell></row><row><cell>OneStepAttn</cell><cell cols="7">100 100 100 100 100 100 0.1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>-Step 2</cell><cell cols="2">15.9 0</cell><cell>0</cell><cell cols="2">16.9 0</cell><cell>0</cell><cell cols="2">95.5 0</cell><cell>0</cell><cell cols="2">96.2 0</cell><cell>0</cell></row><row><cell>-Step 3</cell><cell cols="7">100 100 100 100 100 99.9 40.3</cell><cell>0</cell><cell>0</cell><cell>45</cell><cell>0</cell><cell>0</cell></row><row><cell>-Sigmoid</cell><cell cols="7">100 100 100 100 100 100 0</cell><cell>0</cell><cell>0</cell><cell>22</cell><cell>0</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Input-output examples for DeDupe and PosRetrieve</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="2">DeDupe</cell><cell></cell><cell></cell><cell cols="2">PosRetrieve</cell></row><row><cell cols="2">(Length Splits) IID</cell><cell>15</cell><cell>30</cell><cell cols="2">100 IID</cell><cell cols="3">15 30 100</cell></row><row><cell>Content</cell><cell>98.7</cell><cell>37.1</cell><cell>0</cell><cell>0</cell><cell cols="2">100 0</cell><cell>0</cell><cell>0</cell></row><row><cell>Bi-Relative</cell><cell cols="2">99.9 68</cell><cell>0</cell><cell>0</cell><cell cols="2">100 0</cell><cell>0</cell><cell>0</cell></row><row><cell>LocAttn</cell><cell>99.6</cell><cell>96.9</cell><cell>71.4</cell><cell>0</cell><cell cols="2">96.3 0</cell><cell>0</cell><cell>0</cell></row><row><cell>OneStepAttn</cell><cell>94.1</cell><cell>51.</cell><cell>0</cell><cell>0</cell><cell cols="2">70.6 0</cell><cell>0</cell><cell>0</cell></row><row><cell>MonoAttn</cell><cell>99.7</cell><cell cols="3">98.3 94.3 75</cell><cell>79</cell><cell>0</cell><cell>0</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Accuracy of the models on different length generalization splits in different algorithmic diagnostic/probing tasks. We present the median of five runs on different seeds. We bold the best results.</figDesc><table><row><cell>Model</cell><cell></cell><cell>Copy</cell><cell></cell><cell cols="3">Reverse Copy</cell><cell></cell><cell>Lookup</cell><cell></cell><cell cols="3">Reverse Lookup</cell></row><row><cell>(Length Splits)</cell><cell>15</cell><cell>30</cell><cell>100</cell><cell>15</cell><cell>30</cell><cell>100</cell><cell>7</cell><cell>9</cell><cell>11</cell><cell>7</cell><cell>9</cell><cell>11</cell></row><row><cell>Bi-ROPE</cell><cell>100</cell><cell cols="2">100 0</cell><cell>100</cell><cell cols="2">100 0</cell><cell>100</cell><cell cols="2">100 100</cell><cell cols="2">89.5 1.6</cell><cell>0.2</cell></row><row><cell>Mix LocAttn</cell><cell>99.8</cell><cell>0</cell><cell>0</cell><cell>1.5</cell><cell>0</cell><cell>0</cell><cell>8.6</cell><cell>0</cell><cell>0</cell><cell>7.5</cell><cell>0</cell><cell>0</cell></row><row><cell>LocAttn S</cell><cell>100</cell><cell cols="2">26.6 0</cell><cell>100</cell><cell cols="2">74.1 0</cell><cell>100</cell><cell cols="2">46.3 0</cell><cell cols="2">99.6 0.1</cell><cell>0</cell></row><row><cell>Mix LocAttn S</cell><cell>99.7</cell><cell>0</cell><cell>0</cell><cell>85.9</cell><cell>0</cell><cell>0</cell><cell>33.5</cell><cell>0</cell><cell>0</cell><cell>3.5</cell><cell>0</cell><cell>0</cell></row><row><cell>Mix LocAttn S PR</cell><cell>83.6</cell><cell>0</cell><cell>0</cell><cell>99.7</cell><cell cols="2">10.5 0</cell><cell>32.9</cell><cell>0</cell><cell>0</cell><cell>1.1</cell><cell>0</cell><cell>0</cell></row><row><cell>Mix OneStepAttn</cell><cell>2.6</cell><cell>0</cell><cell>0</cell><cell>0.4</cell><cell>0</cell><cell>0</cell><cell>100</cell><cell cols="2">71.7 1.6</cell><cell cols="3">100 79.1 8.1</cell></row><row><cell cols="2">Mix OneStepAttn PR 100</cell><cell cols="3">100 100 100</cell><cell cols="3">100 100 100</cell><cell cols="4">100 47.8 57.5 0</cell><cell>0</cell></row><row><cell>Mix MonoAttn</cell><cell cols="2">99.95 5</cell><cell>0</cell><cell>100</cell><cell cols="2">50.8 0</cell><cell>94.5</cell><cell>0</cell><cell>0</cell><cell cols="2">53.5 0</cell><cell>0</cell></row><row><cell>Mix MonoAttn PR</cell><cell>100</cell><cell cols="3">99.2 86.7 100</cell><cell cols="3">100 99.7 15.3</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>RMonoAttn</cell><cell>100</cell><cell cols="3">100 100 100</cell><cell cols="3">100 99.9 64.3</cell><cell>0</cell><cell>0</cell><cell cols="2">72.8 0</cell><cell>0</cell></row><row><cell>Mix RMonoAttn</cell><cell>31.1</cell><cell>0</cell><cell>0</cell><cell>100</cell><cell cols="2">66.3 0</cell><cell>99.8</cell><cell>0</cell><cell>0</cell><cell cols="2">41.4 0</cell><cell>0</cell></row><row><cell>Mix RMonoAttn PR</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>100</cell><cell cols="3">100 100 39.2</cell><cell>0</cell><cell>0</cell><cell>4.3</cell><cell>0</cell><cell>0</cell></row><row><cell>Model</cell><cell></cell><cell>ReCopy</cell><cell></cell><cell cols="3">Reverse ReCopy</cell><cell></cell><cell cols="2">Inv ReCopy</cell><cell cols="3">Inv Reverse ReCopy</cell></row><row><cell>(Length Splits)</cell><cell>15</cell><cell>30</cell><cell>100</cell><cell>15</cell><cell>30</cell><cell>100</cell><cell>15</cell><cell>30</cell><cell>100</cell><cell>15</cell><cell>30</cell><cell>100</cell></row><row><cell>Bi-ROPE</cell><cell>43.4</cell><cell>0</cell><cell>0</cell><cell>39</cell><cell>0</cell><cell>0</cell><cell>56.0</cell><cell>0</cell><cell>0</cell><cell cols="2">53.1 0</cell><cell>0</cell></row><row><cell>Mix LocAttn</cell><cell>36.1</cell><cell>0</cell><cell>0</cell><cell>30.4</cell><cell>0</cell><cell>0</cell><cell>99.6</cell><cell cols="2">65.1 0</cell><cell cols="2">98.6 24.1</cell><cell>0</cell></row><row><cell>LocAttn S</cell><cell>98.7</cell><cell>5.2</cell><cell>0</cell><cell>99.9</cell><cell>1.4</cell><cell>0</cell><cell>100</cell><cell cols="2">99.3 91.1</cell><cell cols="2">99.9 99.3</cell><cell>91.8</cell></row><row><cell>Mix Location S</cell><cell>99.7</cell><cell>0</cell><cell>0</cell><cell>99.6</cell><cell>0</cell><cell>0</cell><cell>98.9</cell><cell cols="2">66.6 0</cell><cell cols="2">98.8 57.7</cell><cell>0</cell></row><row><cell>Mix Location S PR</cell><cell>99.6</cell><cell>0.4</cell><cell>0</cell><cell>100</cell><cell cols="2">61.4 0</cell><cell>99.8</cell><cell cols="2">98.6 88</cell><cell cols="2">99.6 98.1</cell><cell>84.8</cell></row><row><cell>Mix OneStepAttn</cell><cell>43.5</cell><cell>0</cell><cell>0</cell><cell>87.1</cell><cell>0</cell><cell>0</cell><cell>10.4</cell><cell>0</cell><cell>0</cell><cell cols="2">5.45 0</cell><cell>0</cell></row><row><cell cols="2">Mix OneStepAttn PR 99.9</cell><cell cols="3">88.4 30.2 100</cell><cell cols="3">100 100 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Mix MonoAttn</cell><cell>7.09</cell><cell>0</cell><cell>0</cell><cell cols="2">99.85 0</cell><cell>0</cell><cell>99.8</cell><cell cols="2">82.4 0</cell><cell cols="2">100 82.2</cell><cell>0</cell></row><row><cell>Mix MonoAttn PR</cell><cell>100</cell><cell cols="3">100 100 100</cell><cell>53</cell><cell>0.5</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="2">89.6 77.6</cell><cell>42.1</cell></row><row><cell>RMonoAttn</cell><cell>100</cell><cell cols="3">100 99.9 0</cell><cell>0</cell><cell>0</cell><cell>100</cell><cell cols="4">100 99.1 100 100</cell><cell>99</cell></row><row><cell>Mix RMonoAttn</cell><cell>98.1</cell><cell>0</cell><cell>0</cell><cell>23.4</cell><cell>0</cell><cell>0</cell><cell cols="3">99.85 70.5 0</cell><cell>100</cell><cell>83.8</cell><cell>0</cell></row><row><cell>Mix RMonoAttn PR</cell><cell>100</cell><cell cols="3">90.8 51.2 100</cell><cell cols="3">100 100 0</cell><cell>0</cell><cell>0</cell><cell>40</cell><cell>0</cell><cell>0</cell></row><row><cell>Model</cell><cell cols="3">SCAN (Length Split)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bi-ROPE</cell><cell cols="2">10.46 ± 3.78</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LocAttn</cell><cell cols="3">24.56 ± 17.51</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LocAttn S</cell><cell cols="2">25.12 ± 6.30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mix LocAttn S</cell><cell cols="3">27.55 ± 10.10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mix LocAttn S PR</cell><cell cols="2">19.8 ± 3.26</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>OneStepAttn</cell><cell cols="2">15.38 ± 0.42</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Mix OneStepAttn PR 17.67 ± 3.54</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MonoAttn</cell><cell cols="2">14.98 ± 0.15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mix MonoAttn PR</cell><cell cols="3">26.68 ± 10.27</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RMonoAttn</cell><cell cols="2">15.28 ± 1.47</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mix RMonoAttn</cell><cell cols="2">22.92 ± 6.39</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mix RMonoAttn PR</cell><cell cols="3">27.99 ± 11.26</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Computer Science, University of Illinois Chicago. Correspondence to: Jishnu Ray Chowdhury &lt;jraych2@uic.edu&gt;, Cornelia Caragea &lt;cornelia@uic.edu&gt;.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://github.com/i-machine-think/ machine-tasks</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p><ref type="bibr" target="#b12">Dubois et al. (2020)</ref> used a GRU for f l . However, in our experiments we removed it because we did not find it effective.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_3"><p>By itself, without content attention mixing, OneStep is monotonic because in it, the center of focus can only move forward with time.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_4"><p>In future, it can be better to have g dependent on the input encoding such as e cls in case we want a multi-tasking model.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_5"><p>Although, we should note that despite their simplicity, the tasks still have been difficult to solve perfectly<ref type="bibr" target="#b12">(Dubois et al., 2020;</ref><ref type="bibr" target="#b11">Dehghani et al., 2019;</ref><ref type="bibr" target="#b23">Liang et al., 2021)</ref> </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research is supported in part by <rs type="funder">NSF</rs> <rs type="programName">CAREER</rs> award #<rs type="grantNumber">1802358</rs>, <rs type="funder">NSF</rs> <rs type="grantName">IIS</rs> award #<rs type="grantNumber">2107518</rs>, and <rs type="funder">UIC Discovery Partners Institute (DPI)</rs> award. Any opinions, findings, and conclusions expressed here are those of the authors and do not necessarily reflect the views of <rs type="institution">NSF</rs> or DPI. We thank our anonymous reviewers for their constructive feedback.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_2zBadkd">
					<idno type="grant-number">1802358</idno>
					<orgName type="program" subtype="full">CAREER</orgName>
				</org>
				<org type="funding" xml:id="_YyBudKS">
					<idno type="grant-number">2107518</idno>
					<orgName type="grant-name">IIS</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OneStepAttn Inv ReCopy</head><p>Input: 1 2 5 5 5 5 5 5 7 7 7 7 7 2 9 9 9 9 9 9 9 9 9 9 3 7 7 7 7 7 9 9 9 9 9 4 4 4 5 5 5 3 3 4 4 4 0 1 1 4 4 4 Oracle: 1 2 5 5 7 2 9 9 3 7 9 4 5 3 3 4 0 1 1 4 Prediction: 1 2 5 5 7 2 9 9 3 9 7 3 4 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OneStepAttn Inv ReCopy</head><p>Input: 9 9 9 9 9 2 1 1 6 6 6 6 6 6 3 7 7 7 7 7 6 6 6 8 8 8 8 8 1 5 5 5 4 4 4 3 7 7 7 7 7 4 4 4 5 5 5 7 7 7 7 7 3 7 7 7 7 7 Oracle: 9 2 1 1 6 6 3 7 6 8 1 5 4 3 7 4 5 7 3 7 Prediction: 9 2 1 1 6 6 3 7 6 8 1 4 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MonoAttn Reverse Lookup</head><p>Input: t4 t4 t1 t6 t5 t2 t6 t3 t4 100 . window). Most of the approaches mentioned in this paragraph, however, involve emphasis on relative distances. As we find in our investigations based on two popular representatives of such forms of attention -Relative attention <ref type="bibr" target="#b10">(Dai et al., 2019)</ref> and ROPE <ref type="bibr" target="#b38">(Su et al., 2021)</ref> -they tend to struggle on tasks like ReCopy where the ideal relative distance of attention position can be arbitrarily big and can vary with every timestep. <ref type="bibr" target="#b33">Press et al. (2022)</ref> introduced a new positional encoding technique for transformer-based decoders for better length generalization on language modeling. They add a bias towards locality for that purpose. However, the advantage of locality bias in the contexts of seq2seq tasks that we consider here are less clear given that the ideal position of attention can be arbitrarily distant from the current timestep of decoding in our tasks. Transformers can iteratively transfer information depth-wise through local operations but that will be also limited by the maximum layer depth. However, allowing adaptive layers <ref type="bibr" target="#b36">(Schmidhuber, 2012;</ref><ref type="bibr" target="#b15">Graves, 2016;</ref><ref type="bibr">Bai et al., 2019;</ref><ref type="bibr" target="#b4">Banino et al., 2021)</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploring length generalization in large language models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Andreassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Ramasesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Slone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=zSkYVeX7bC4" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.0473" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<title level="s">Conference Track Proceedings</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep equilibrium models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><surname>Curran Associates</surname></persName>
		</author>
		<author>
			<persName><surname>Inc</surname></persName>
		</author>
		<ptr target="https://proceedings" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pondernet: Learning to ponder</title>
		<author>
			<persName><forename type="first">A</forename><surname>Banino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Balaguer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<idno>, abs/2107.05407</idno>
		<ptr target="https://arxiv.org/abs/2107.05407" />
	</analytic>
	<monogr>
		<title level="m">ICML Workshop</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Natural Language Processing with Python</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>O&apos;Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The eccentricity effect: Target eccentricity affects performance on conjunction searches</title>
		<author>
			<persName><forename type="first">M</forename><surname>Carrasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Evert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Katz</surname></persName>
		</author>
		<idno type="DOI">10.3758/bf03208380</idno>
		<ptr target="https://doi.org/10.3758/bf03208380" />
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1241" to="1261" />
			<date type="published" when="1995-11">November 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutions and selfattention: Re-interpreting relative positions in pre-trained language models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.333</idno>
		<ptr target="https://aclanthology.org/2021.acl-long.333" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<title level="s">Association for Computational Linguistics</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021-08">August 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4322" to="4333" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">¸</forename><surname>Gülc ¸ehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>, abs/1412.3555</idno>
		<ptr target="http://arxiv.org/abs/1412.3555" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The neural data router: Adaptive control flow in transformers improves systematic generalization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Csordás</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=KBQP4A_J1K" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1285</idno>
		<ptr target="https://aclanthology.org/P19-1285" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Association for Computational Lin</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07">July 2019</date>
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Universal transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HyzdRiR9Y7" />
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Location Attention for Extrapolation to Longer Sequences</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hupkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bruni</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.39</idno>
		<ptr target="https://aclanthology.org/2020.acl-main" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Association for Computational Linguis</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="403" to="413" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Position Information in Transformers: An Overview</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dufter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli_a_00445</idno>
		<ptr target="https://doi.org/10.1162/coli_a_00445" />
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<idno type="ISSN">0891-2017</idno>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="733" to="763" />
			<date type="published" when="2022-09">09 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">LISA: Learning interpretable skill abstractions from language</title>
		<author>
			<persName><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vaidyanath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=XZhipvOUBB" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adaptive computation time for recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno>ArXiv, abs/1603.08983</idno>
		<ptr target="http://arxiv.org/abs/1603.08983" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Neural turing machines</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno>ArXiv, abs/1410.5401</idno>
		<ptr target="http://arxiv.org/abs/1410.5401" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural gpus learn algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.08228" />
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rethinking positional encoding in language pre-training</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=09-528y2Fgf" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Measuring compositional generalization: A comprehensive method on realistic data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schärli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Buisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Furrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kashubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Momchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sinopalnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Stafiniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tihon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsarkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Zee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SygcCnNKwr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Uncontrolled lexical exposure leads to overestimation of compositional generalization in pretrained models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smolensky</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2212.10769" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v80/lake18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07-15">10-15 Jul 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2873" to="2882" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Bart</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
		<ptr target="https://aclanthology.org/2020.acl-main.703" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Out-ofdistribution generalization with deep equilibrium models</title>
		<author>
			<persName><forename type="first">K.-P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cape: Encoding relative positions with continuous augmented positional embeddings</title>
		<author>
			<persName><forename type="first">T</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rogozhnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="16079" to="16092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><surname>Curran Associates</surname></persName>
		</author>
		<author>
			<persName><surname>Inc</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/file/865bf46435bd84fa5d89f64cf3ba7347-Paper.pdf" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Memorize or generalize? searching for a compositional RNN in a haystack</title>
		<author>
			<persName><forename type="first">A</forename><surname>Liska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<idno>, abs/1802.06467</idno>
		<ptr target="http://arxiv.org/abs/1802.06467" />
	</analytic>
	<monogr>
		<title level="m">ICML Workshop</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stable, fast and accurate: Kernelized attention with relative positional encoding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="22795" to="22807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><surname>Curran Associates</surname></persName>
		</author>
		<author>
			<persName><surname>Inc</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/file/c0f168ce8900fa56e57789e2a2f2c9d0-Paper.pdf" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Your transformer may not be as powerful as you expect</title>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=NQFFNdsOGD" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1166</idno>
		<ptr target="https://aclanthology.org/D15-1166" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<title level="s">Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09">September 2015</date>
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The eos decision and length extrapolation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Show your work: Scratchpads for intermediate computation with language models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Andreassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno>ArXiv, abs/2112.00114</idno>
		<ptr target="https://arxiv.org/abs/2112.00114" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Train short, test long: Attention with linear biases enables input length extrapolation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=R8sQPpGCv0" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Explore better relative position embeddings from encoding perspective for transformer models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mo</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.237</idno>
		<ptr target="https://aclanthology.org/2021.emnlp-main.237" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<title level="s">Association for Computational Linguistics</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-11">November 2021</date>
			<biblScope unit="page" from="2989" to="2997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v21/20-074.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Self-delimiting neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>arXiv, abs/1210.0118</idno>
		<ptr target="http://arxiv.org/abs/1210.0118" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2074</idno>
		<ptr target="https://aclanthology.org/N18-2074" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Association for Computational Linguistics</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno>ArXiv, abs/2104.09864</idno>
		<ptr target="https://arxiv.org/abs/2104.09864" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Benhaim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.10554</idno>
		<title level="m">A length-extrapolatable transformer</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><surname>Curran Associates</surname></persName>
		</author>
		<author>
			<persName><surname>Inc</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><surname>Curran Associates</surname></persName>
		</author>
		<author>
			<persName><surname>Inc</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Encoding word order in complex embeddings</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Simonsen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hke-WTVtwr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The case for translationinvariant self-attention in transformer-based language models</title>
		<author>
			<persName><forename type="first">U</forename><surname>Wennberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Henter</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-short.18</idno>
		<ptr target="https://aclanthology.org/2021.acl-short" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<title level="s">Association for Computational Linguis</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021-08">August 2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="130" to="140" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">DA-transformer: Distanceaware transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.166</idno>
		<ptr target="https://aclanthology.org/2021.naacl-main.166" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Association for Computational Linguistics</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021-06">June 2021</date>
			<biblScope unit="page" from="2059" to="2068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SkVhlh09tX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Modeling localness for self-attention networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1475</idno>
		<ptr target="https://aclanthology.org/D18-1475" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<title level="s">Association for Computational Linguistics</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11">October-November 2018</date>
			<biblScope unit="page" from="4449" to="4458" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
