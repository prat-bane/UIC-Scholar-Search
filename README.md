# UIC-Scholar-Search
This project is a data processing pipeline designed to build a powerful knowledge graph for a search engine focused on academic papers by UIC researchers.

The pipeline ingests PDF files of academic papers, processes them to extract structured data and semantic embeddings, and loads this information into a Neo4j graph database. The resulting graph is optimized for advanced search capabilities, including both traditional keyword search and modern semantic search.

## Features

*   **PDF Processing**: Automatically processes PDF files using GROBID to extract structured metadata, text content, and author information.
*   **Vector Embeddings**: Generates 384-dimensional vector embeddings for text sections, enabling powerful semantic search to find conceptually similar content.
*   **Knowledge Graph Construction**: Builds a rich graph in Neo4j connecting `Paper` nodes to their `UIC_Author`s, `Topic`s, and text `Section`s.
*   **Optimized for Search**: Creates multiple database indexes to ensure high-performance queries:
    *   A **vector index** on text section embeddings for semantic search.
    *   **Full-text indexes** on author names, topics, and section text for fast keyword search.
*   **Idempotent Data Loading**: The loading script can be run multiple times without creating duplicate data, ensuring the graph remains clean and consistent.

## The Data Pipeline

The processing pipeline consists of two main stages, orchestrated by two Python scripts.

### 1. PDF to JSON Extraction (`app.py`)

This script is the entry point for the pipeline. It performs the following steps:

1.  **Cleans Directories**: Removes any old files from the `output_xml/` and `paper_json/` directories to ensure a fresh start.
2.  **Processes PDFs**: Takes all PDF files from the `input/` directory.
3.  **Uses GROBID**: Leverages a GROBID client to parse the PDFs, generating intermediate XML files in `output_xml/`.
4.  **Creates JSON**: Converts the structured XML into JSON files, which are saved in the `paper_json/` directory. Each JSON file represents a single paper and contains:
    *   Paper metadata (ID, title, year, link).
    *   A list of UIC-affiliated authors with their name, department, and title.
    *   A list of keywords.
    *   A list of text sections (e.g., Abstract, Introduction), each with its full text and a pre-calculated 384-dimensional vector embedding.

### 2. JSON to Neo4j Loading (`load_to_neo4j.py`)

This script populates the Neo4j database from the generated JSON files.

1.  **Sets up the Database**: It first ensures that all necessary constraints and indexes are created in Neo4j. This is a critical step for data integrity and query performance.
2.  **Loads Data**: It iterates through every `.json` file in the `paper_json/` directory.
3.  **Builds the Graph**: For each paper, it executes a single, efficient Cypher query to create the following graph structure:
    *   A central **`(Paper)`** node with metadata.
    *   **`(UIC_Author:Author)`** nodes for each UIC author, containing their department and title.
    *   An **`[:AUTHORED]`** relationship from each author to the paper.
    *   **`(Topic)`** nodes for each keyword.
    *   A **`[:HAS_TOPIC]`** relationship from the paper to its topics.
    *   **`(Section)`** nodes for each text section, containing the text and its vector embedding.
    *   A **`[:HAS_SECTION]`** relationship from the paper to its sections.

## How to Use

1.  **Prerequisites**:
    *   A running Neo4j 4.4+ instance (the vector index requires this version). Update the connection details in `load_to_neo4j.py` if they differ from the defaults.
    *   A running GROBID service.
    *   Python environment with necessary libraries (`neo4j`, etc.).

2.  **Add Papers**: Place the PDF files of the academic papers you want to process into the `input/` directory.

3.  **Run the Extraction**:
    ```bash
    python app.py
    ```
    This will populate the `paper_json/` directory with structured data from your PDFs.

4.  **Load into Neo4j**:
    ```bash
    python load_to_neo4j.py
    ```
    This will connect to your Neo4j database, set up the required schema, and load all the processed paper data into the graph.

After these steps, your Neo4j database will be populated and ready for querying.

---

*This README was generated by Gemini Code Assist.*
